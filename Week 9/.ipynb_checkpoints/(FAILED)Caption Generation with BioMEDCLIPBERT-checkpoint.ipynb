{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17450d67-868c-4170-8f70-28f015760b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "train_caption_file = \"../Datasets/ROCO2/train_captions.csv\"\n",
    "train_image_folder = \"../Datasets/ROCO2/train_images/train/\"\n",
    "test_caption_file = \"../Datasets/ROCO2/test_captions.csv\"\n",
    "test_image_folder = \"../Datasets/ROCO2/test_images/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baae5b35-536b-4397-a1fd-e000e9388cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda3\\envs\\MEDCAT\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "G:\\anaconda3\\envs\\MEDCAT\\Lib\\site-packages\\open_clip\\factory.py:129: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=map_location)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'open_clip' has no attribute 'get_preprocess'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 133\u001b[0m\n\u001b[0;32m    130\u001b[0m     train_model(dataloader, model_manager, device)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 133\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[2], line 122\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    120\u001b[0m biomedclip_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    121\u001b[0m gpt2_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 122\u001b[0m model_manager \u001b[38;5;241m=\u001b[39m ModelManager(biomedclip_model_name, gpt2_model_name)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# Device setup\u001b[39;00m\n\u001b[0;32m    125\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 45\u001b[0m, in \u001b[0;36mModelManager.__init__\u001b[1;34m(self, biomedclip_model_name, gpt2_model_name)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiomedclip_model \u001b[38;5;241m=\u001b[39m open_clip\u001b[38;5;241m.\u001b[39mcreate_model(biomedclip_model_name)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# For transformation, use open_clip's recommended preprocess\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess \u001b[38;5;241m=\u001b[39m open_clip\u001b[38;5;241m.\u001b[39mget_preprocess(biomedclip_model_name)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpt2_model \u001b[38;5;241m=\u001b[39m GPT2LMHeadModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(gpt2_model_name)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpt2_tokenizer \u001b[38;5;241m=\u001b[39m GPT2Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(gpt2_model_name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'open_clip' has no attribute 'get_preprocess'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import open_clip\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define Dataset Class\n",
    "class MedicalImageCaptioningDataset(Dataset):\n",
    "    def __init__(self, caption_file, image_folder, tokenizer, transform=None):\n",
    "        self.captions_df = pd.read_csv(caption_file)\n",
    "        self.image_folder = image_folder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.captions_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.captions_df.iloc[idx, 0]\n",
    "        caption = self.captions_df.iloc[idx, 1]\n",
    "        \n",
    "        image_path = os.path.join(self.image_folder, f\"{image_id}.jpg\")\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        caption_tokens = self.tokenizer(caption, return_tensors='pt', padding='max_length', truncation=True, max_length=128)\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'caption': caption_tokens['input_ids'].squeeze(0),\n",
    "            'attention_mask': caption_tokens['attention_mask'].squeeze(0)\n",
    "        }\n",
    "\n",
    "# Define Model Manager Class\n",
    "class ModelManager:\n",
    "    def __init__(self, biomedclip_model_name, gpt2_model_name):\n",
    "        # Initialize BiomedCLIP model\n",
    "        self.biomedclip_model = open_clip.create_model(biomedclip_model_name)\n",
    "        # For transformation, use open_clip's recommended preprocess\n",
    "        self.preprocess = open_clip.get_preprocess(biomedclip_model_name)\n",
    "        self.gpt2_model = GPT2LMHeadModel.from_pretrained(gpt2_model_name)\n",
    "        self.gpt2_tokenizer = GPT2Tokenizer.from_pretrained(gpt2_model_name)\n",
    "        self.optimizer = AdamW(list(self.biomedclip_model.parameters()) + list(self.gpt2_model.parameters()), lr=1e-4)\n",
    "    \n",
    "    def extract_image_embeddings(self, images):\n",
    "        self.biomedclip_model.eval()\n",
    "        embeddings = []\n",
    "        with torch.no_grad():\n",
    "            for image in images:\n",
    "                image = self.preprocess(image).unsqueeze(0)  # Add batch dimension\n",
    "                embedding = self.biomedclip_model.encode_image(image)\n",
    "                embeddings.append(embedding)\n",
    "            return torch.cat(embeddings)\n",
    "    \n",
    "    def generate_captions(self, image_embeddings, captions):\n",
    "        self.gpt2_model.eval()\n",
    "        outputs = self.gpt2_model.generate(\n",
    "            input_ids=captions,\n",
    "            attention_mask=None,\n",
    "            max_length=128\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "def train_model_mixed_precision(model, dataloader, epochs=5, lr=1e-4):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=model.gpt2_tokenizer.pad_token_id)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # GradScaler for mixed precision\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for image_tensor, caption_tensor in dataloader:\n",
    "            image_tensor = image_tensor.to(device)\n",
    "            caption_tensor = caption_tensor.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Mixed precision forward pass\n",
    "            with autocast():\n",
    "                outputs = model(image_tensor, caption_tensor[:, :-1])  # Exclude last token for target\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(logits.view(-1, logits.size(-1)), caption_tensor[:, 1:].reshape(-1))\n",
    "\n",
    "            # Backward pass with scaled gradients\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Step optimizer with scaled gradients\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(dataloader):.4f}')\n",
    "\n",
    "    print(\"Training Finished with Mixed Precision!\")\n",
    "\n",
    "def main():\n",
    "    # Paths\n",
    "    train_caption_file = \"../Datasets/ROCO2/train_captions.csv\"\n",
    "    train_image_folder = \"../Datasets/ROCO2/train_images/train/\"\n",
    "    \n",
    "    # Initialize tokenizer and transformations\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = MedicalImageCaptioningDataset(train_caption_file, train_image_folder, tokenizer, transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "    \n",
    "    # Initialize model manager\n",
    "    biomedclip_model_name = 'hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224'\n",
    "    gpt2_model_name = 'gpt2'\n",
    "    model_manager = ModelManager(biomedclip_model_name, gpt2_model_name)\n",
    "    \n",
    "    # Device setup\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model_manager.biomedclip_model.to(device)\n",
    "    model_manager.gpt2_model.to(device)\n",
    "    \n",
    "    # Train the model\n",
    "    train_model(dataloader, model_manager, device)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
