{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3466dabd-0180-492b-a16f-2877b696946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1526bfb-5cce-40c7-b264-ee5127f74e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce RTX 3070\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d29c58fc-816f-439e-8530-fa81b08e775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_folder, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_folder, self.data.iloc[idx, 0] + '.jpg')  # Pick first column and add .jpg\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        caption = self.data.iloc[idx, 1]  # Assuming 'Caption' is the second column\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, caption\n",
    "\n",
    "def preprocess_images(images, preprocess_func):\n",
    "    \"\"\"Preprocess images using the given preprocessing function.\"\"\"\n",
    "    preprocessed_images = []\n",
    "    for img in images:\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            # Convert tensor to PIL image for preprocessing\n",
    "            img = transforms.ToPILImage()(img)\n",
    "        preprocessed_images.append(preprocess_func(img))\n",
    "    return torch.stack(preprocessed_images)\n",
    "\n",
    "def tokenize_captions(captions, tokenizer, context_length):\n",
    "    \"\"\"Tokenize captions.\"\"\"\n",
    "    # Add padding token if not already present\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "    return tokenizer(captions, padding=True, truncation=True, max_length=context_length, return_tensors='pt')\n",
    "\n",
    "def train_model(model_manager, dataloader, num_epochs):\n",
    "    \"\"\"Train the model with progress tracking and time estimation.\"\"\"\n",
    "    from tqdm import tqdm\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    import time\n",
    "    \n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        for batch_idx, (images, captions) in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch+1}\")):\n",
    "            # Extract image embeddings\n",
    "            image_embeddings_gpt2 = model_manager.extract_image_embeddings(images)\n",
    "            \n",
    "            # Tokenize captions\n",
    "            tokenized_captions = tokenize_captions(captions, model_manager.gpt2_tokenizer, context_length=256)\n",
    "            \n",
    "            # Generate captions\n",
    "            prompts = [\"Medical image description: \"] * len(images)\n",
    "            generated_captions = model_manager.generate_captions(image_embeddings_gpt2, prompts)\n",
    "            \n",
    "            # Log generated captions to TensorBoard\n",
    "            for i, caption in enumerate(generated_captions):\n",
    "                writer.add_text(f'Generated Caption/{batch_idx * len(images) + i}', caption)\n",
    "        \n",
    "        epoch_end_time = time.time()\n",
    "        epoch_duration = epoch_end_time - epoch_start_time\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} took {epoch_duration:.2f} seconds\")\n",
    "        \n",
    "        # Log epoch duration to TensorBoard\n",
    "        writer.add_scalar('Epoch Duration', epoch_duration, epoch)\n",
    "    \n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "518bdb53-ac0f-4951-8efe-8cbc309869eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelManager:\n",
    "    def __init__(self, biomedclip_model_name: str, gpt2_model_name: str):\n",
    "        # Load BiomedCLIP model and preprocess functions\n",
    "        self.biomedclip_model, self.preprocess_train, self.preprocess_val = open_clip.create_model_and_transforms(biomedclip_model_name)\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.biomedclip_model.to(self.device)\n",
    "        \n",
    "        # Load GPT-2 model and tokenizer\n",
    "        self.gpt2_model = GPT2LMHeadModel.from_pretrained(gpt2_model_name).to(self.device)\n",
    "        self.gpt2_tokenizer = GPT2Tokenizer.from_pretrained(gpt2_model_name)\n",
    "        \n",
    "        # Set the correct output dimension for image embeddings\n",
    "        self.image_embeddings_dim = 512  # Based on observed feature shape\n",
    "        \n",
    "        # Initialize projection layer\n",
    "        self.image_embeddings_projected = torch.nn.Linear(self.image_embeddings_dim, self.gpt2_model.config.n_embd).to(self.device)\n",
    "\n",
    "    def extract_image_embeddings(self, images):\n",
    "        \"\"\"Extract image embeddings from BiomedCLIP.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            preprocessed_images = preprocess_images(images, self.preprocess_val)  # Use preprocess_val for validation images\n",
    "            image_embeddings = self.biomedclip_model.encode_image(preprocessed_images.to(self.device))\n",
    "            return self.image_embeddings_projected(image_embeddings)\n",
    "    \n",
    "    def generate_captions(self, image_embeddings_gpt2, prompts, num_return_sequences=1):\n",
    "        \"\"\"Generate captions using GPT-2.\"\"\"\n",
    "        self.gpt2_model.eval()\n",
    "        generated_captions = []\n",
    "        \n",
    "        for i, embedding in enumerate(image_embeddings_gpt2):\n",
    "            prompt = prompts[i]  # Use the corresponding prompt for each image embedding\n",
    "            \n",
    "            # Prepare inputs\n",
    "            input_ids = self.gpt2_tokenizer(prompt, return_tensors='pt').input_ids.to(self.device)\n",
    "            input_embeds = self.gpt2_model.transformer.wte(input_ids)\n",
    "            \n",
    "            # Expand embedding dimensions to match input_embeds\n",
    "            embedding_expanded = embedding.unsqueeze(0).unsqueeze(1)  # Shape: (1, 1, embedding_dim)\n",
    "            \n",
    "            # Ensure input_embeds and embedding_expanded have compatible dimensions\n",
    "            combined_embeds = torch.cat((embedding_expanded.expand(input_embeds.size(0), -1, -1), input_embeds), dim=1)\n",
    "            \n",
    "            # Create attention mask\n",
    "            attention_mask = torch.ones(combined_embeds.shape[:-1]).to(self.device)\n",
    "            \n",
    "            # Generate captions\n",
    "            generated_ids = self.gpt2_model.generate(\n",
    "                inputs_embeds=combined_embeds,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=50,\n",
    "                pad_token_id=self.gpt2_tokenizer.pad_token_id,\n",
    "                num_return_sequences=num_return_sequences\n",
    "            )\n",
    "            \n",
    "            # Decode generated captions\n",
    "            generated_caption = self.gpt2_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "            generated_captions.append(generated_caption)\n",
    "        \n",
    "        return generated_captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3d77bb3-695d-44e0-be92-4077a880b3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CustomTextCLIP:\n\tMissing key(s) in state_dict: \"text.transformer.embeddings.position_ids\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Initialize model manager\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m model_manager \u001b[38;5;241m=\u001b[39m ModelManager(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m, in \u001b[0;36mModelManager.__init__\u001b[1;34m(self, biomedclip_model_name, gpt2_model_name)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, biomedclip_model_name: \u001b[38;5;28mstr\u001b[39m, gpt2_model_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Load BiomedCLIP model and preprocess functions\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiomedclip_model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_val \u001b[38;5;241m=\u001b[39m open_clip\u001b[38;5;241m.\u001b[39mcreate_model_and_transforms(biomedclip_model_name)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiomedclip_model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mG:\\anaconda3\\Lib\\site-packages\\open_clip\\factory.py:399\u001b[0m, in \u001b[0;36mcreate_model_and_transforms\u001b[1;34m(model_name, pretrained, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, image_mean, image_std, image_interpolation, image_resize_mode, aug_cfg, pretrained_image, pretrained_hf, cache_dir, output_dict, **model_kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_model_and_transforms\u001b[39m(\n\u001b[0;32m    376\u001b[0m         model_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    377\u001b[0m         pretrained: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    395\u001b[0m ):\n\u001b[0;32m    396\u001b[0m     force_preprocess_cfg \u001b[38;5;241m=\u001b[39m merge_preprocess_kwargs(\n\u001b[0;32m    397\u001b[0m         {}, mean\u001b[38;5;241m=\u001b[39mimage_mean, std\u001b[38;5;241m=\u001b[39mimage_std, interpolation\u001b[38;5;241m=\u001b[39mimage_interpolation, resize_mode\u001b[38;5;241m=\u001b[39mimage_resize_mode)\n\u001b[1;32m--> 399\u001b[0m     model \u001b[38;5;241m=\u001b[39m create_model(\n\u001b[0;32m    400\u001b[0m         model_name,\n\u001b[0;32m    401\u001b[0m         pretrained,\n\u001b[0;32m    402\u001b[0m         precision\u001b[38;5;241m=\u001b[39mprecision,\n\u001b[0;32m    403\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m    404\u001b[0m         jit\u001b[38;5;241m=\u001b[39mjit,\n\u001b[0;32m    405\u001b[0m         force_quick_gelu\u001b[38;5;241m=\u001b[39mforce_quick_gelu,\n\u001b[0;32m    406\u001b[0m         force_custom_text\u001b[38;5;241m=\u001b[39mforce_custom_text,\n\u001b[0;32m    407\u001b[0m         force_patch_dropout\u001b[38;5;241m=\u001b[39mforce_patch_dropout,\n\u001b[0;32m    408\u001b[0m         force_image_size\u001b[38;5;241m=\u001b[39mforce_image_size,\n\u001b[0;32m    409\u001b[0m         force_preprocess_cfg\u001b[38;5;241m=\u001b[39mforce_preprocess_cfg,\n\u001b[0;32m    410\u001b[0m         pretrained_image\u001b[38;5;241m=\u001b[39mpretrained_image,\n\u001b[0;32m    411\u001b[0m         pretrained_hf\u001b[38;5;241m=\u001b[39mpretrained_hf,\n\u001b[0;32m    412\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    413\u001b[0m         output_dict\u001b[38;5;241m=\u001b[39moutput_dict,\n\u001b[0;32m    414\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    415\u001b[0m     )\n\u001b[0;32m    417\u001b[0m     pp_cfg \u001b[38;5;241m=\u001b[39m PreprocessCfg(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel\u001b[38;5;241m.\u001b[39mvisual\u001b[38;5;241m.\u001b[39mpreprocess_cfg)\n\u001b[0;32m    419\u001b[0m     preprocess_train \u001b[38;5;241m=\u001b[39m image_transform_v2(\n\u001b[0;32m    420\u001b[0m         pp_cfg,\n\u001b[0;32m    421\u001b[0m         is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    422\u001b[0m         aug_cfg\u001b[38;5;241m=\u001b[39maug_cfg,\n\u001b[0;32m    423\u001b[0m     )\n",
      "File \u001b[1;32mG:\\anaconda3\\Lib\\site-packages\\open_clip\\factory.py:315\u001b[0m, in \u001b[0;36mcreate_model\u001b[1;34m(model_name, pretrained, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, force_preprocess_cfg, pretrained_image, pretrained_hf, cache_dir, output_dict, require_pretrained, **model_kwargs)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m has_hf_hub_prefix:\n\u001b[0;32m    314\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading pretrained \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m weights (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 315\u001b[0m     load_checkpoint(model, checkpoint_path)\n\u001b[0;32m    316\u001b[0m     pretrained_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m require_pretrained \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pretrained_loaded:\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;66;03m# callers of create_model_from_pretrained always expect pretrained weights\u001b[39;00m\n",
      "File \u001b[1;32mG:\\anaconda3\\Lib\\site-packages\\open_clip\\factory.py:176\u001b[0m, in \u001b[0;36mload_checkpoint\u001b[1;34m(model, checkpoint_path, strict)\u001b[0m\n\u001b[0;32m    173\u001b[0m resize_text_pos_embed(state_dict, model)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;66;03m# Finally, load the massaged state_dict into model\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m incompatible_keys \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict, strict\u001b[38;5;241m=\u001b[39mstrict)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m incompatible_keys\n",
      "File \u001b[1;32mG:\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CustomTextCLIP:\n\tMissing key(s) in state_dict: \"text.transformer.embeddings.position_ids\". "
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "train_caption_file = \"../Datasets/ROCO2/train_captions.csv\"\n",
    "train_image_folder = \"../Datasets/ROCO2/train_images/train/\"\n",
    "test_caption_file = \"../Datasets/ROCO2/test_captions.csv\"\n",
    "test_image_folder = \"../Datasets/ROCO2/test_images/test/\"\n",
    "\n",
    "# Define image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = MedicalImageDataset(csv_file=train_caption_file, image_folder=train_image_folder, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "\n",
    "# Initialize model manager\n",
    "model_manager = ModelManager('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224', 'gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5a0aaa-14ac-4f5c-908a-9b3c638c2d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "train_model(model_manager, dataloader, num_epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
