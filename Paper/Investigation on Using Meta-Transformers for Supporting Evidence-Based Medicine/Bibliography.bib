@IEEEtranBSTCTL{IEEEexample:BSTcontrol,
    CTLuse_article_number = "yes",
    CTLuse_paper = "yes",
    CTLuse_forced_etal = "no",
    CTLmax_names_forced_etal = "50",
    CTLnames_show_etal = "50",
    CTLuse_alt_spacing = "yes",
    CTLalt_stretch_factor = "4",
    CTLdash_repeated_names = "yes",
    CTLname_format_string = "{f.~}{vv~}{ll}{, jj}",
    CTLname_latex_cmd = "",
    CTLname_url_prefix = "[Online]. Available:"
 }

@article{zhang2023meta,
  title={Meta-transformer: A unified framework for multimodal learning},
  author={Zhang, Yiyuan and Gong, Kaixiong and Zhang, Kaipeng and Li, Hongsheng and Qiao, Yu and Ouyang, Wanli and Yue, Xiangyu},
  journal={arXiv preprint arXiv:2307.10802},
  year={2023}
}

@article{munikoti2024generalist,
  title={Generalist Multimodal AI: A Review of Architectures, Challenges and Opportunities},
  author={Munikoti, Sai and Stewart, Ian and Horawalavithana, Sameera and Kvinge, Henry and Emerson, Tegan and Thompson, Sandra E and Pazdernik, Karl},
  journal={arXiv preprint arXiv:2406.05496},
  year={2024}
}

@article{wu2023next,
  title={Next-gpt: Any-to-any multimodal llm},
  author={Wu, Shengqiong and Fei, Hao and Qu, Leigang and Ji, Wei and Chua, Tat-Seng},
  journal={arXiv preprint arXiv:2309.05519},
  year={2023}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International Conference on Machine Learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and others},
  journal={arXiv preprint arXiv:2204.14198},
  year={2022}
}

@article{wang2021vlmo,
  title={Vlmo: Unified vision-language pre-training with mixture-of-modality-experts},
  author={Wang, Wenhui and Bao, Hangbo and Dong, Li and Wei, Furu},
  journal={arXiv preprint arXiv:2111.02358},
  year={2021}
}

@article{wang2022unifying,
  title={Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework},
  author={Wang, Peng and Yang, An and Men, Rui and Lin, Junyang and Bai, Shuai and Li, Zhikang and Ma, Jianxin and Zhou, Chang and Zhou, Jingren and Yang, Hongxia},
  journal={arXiv preprint arXiv:2202.03052},
  year={2022}
}

@article{wang2022image,
  title={Image as a foreign language: Beit pretraining for all vision and vision-language tasks},
  author={Wang, Wenhui and Bao, Hangbo and Dong, Li and Bjorck, Johan and Peng, Zhiliang and Liu, Qiang and Aggarwal, Kriti and Mohammed, Owais Khan and Singhal, Saksham and Som, Subhojit and others},
  journal={arXiv preprint arXiv:2208.10442},
  year={2022}
}

@article{lee2024metaswin,
  title={MetaSwin: a unified meta vision transformer model for medical image segmentation},
  author={Lee, Soyeon and Lee, Minhyeok},
  journal={PeerJ Computer Science},
  volume={10},
  pages={e1762},
  year={2024}
}

@inproceedings{li2023fine,
  title={Fine-tuning multimodal llms to follow zero-shot demonstrative instructions},
  author={Li, Juncheng and Pan, Kaihang and Ge, Zhiqi and Gao, Minghe and Ji, Wei and Zhang, Wenqiao and Chua, Tat-Seng and Tang, Siliang and Zhang, Hanwang and Zhuang, Yueting},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{kline2022multimodal,
  title={Multimodal machine learning in precision health},
  author={Kline, Adrienne and Wang, Hanyin and Li, Yikuan and Dennis, Saya and Hutch, Meghan and Xu, Zhenxing and Wang, Fei and Cheng, Feixiong and Luo, Yuan},
  journal={arXiv preprint arXiv:2204.04777},
  year={2022}
}

