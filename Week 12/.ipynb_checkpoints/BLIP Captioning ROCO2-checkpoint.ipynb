{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9be4940b-b967-4b4a-a075-82f78a070345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset, DatasetDict\n",
    "from PIL import Image\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51da99bf-d088-4375-8c04-3cdb2c255fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROCODataLoader:\n",
    "    def __init__(self, image_dir_train, image_dir_val, annotation_file_train, annotation_file_val):\n",
    "        \"\"\"\n",
    "        Initializes the data loader with directories for training and validation images, and annotation files.\n",
    "        :param image_dir_train: Directory where training images are stored.\n",
    "        :param image_dir_val: Directory where validation images are stored.\n",
    "        :param annotation_file_train: Path to the CSV file for training annotations.\n",
    "        :param annotation_file_val: Path to the CSV file for validation annotations.\n",
    "        \"\"\"\n",
    "        self.image_dir_train = image_dir_train\n",
    "        self.image_dir_val = image_dir_val\n",
    "        self.annotation_file_train = annotation_file_train\n",
    "        self.annotation_file_val = annotation_file_val\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Loads and prepares both the training and validation datasets.\n",
    "        :return: A DatasetDict with 'train' and 'validation' datasets.\n",
    "        \"\"\"\n",
    "        train_df = pd.read_csv(self.annotation_file_train)\n",
    "        val_df = pd.read_csv(self.annotation_file_val)\n",
    "\n",
    "        # Add full image paths for training and validation sets\n",
    "        train_df['image_path'] = train_df['ID'].apply(lambda x: os.path.join(self.image_dir_train, x + \".jpg\"))\n",
    "        val_df['image_path'] = val_df['ID'].apply(lambda x: os.path.join(self.image_dir_val, x + \".jpg\"))\n",
    "\n",
    "        # Convert to Hugging Face dataset format\n",
    "        train_dataset = Dataset.from_pandas(train_df)\n",
    "        val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "        # Return a DatasetDict containing both training and validation sets\n",
    "        return DatasetDict({'train': train_dataset, 'validation': val_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "514d9aa2-5727-484f-807d-f02f36565190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples, processor):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset by loading images and processing them using the BLIP processor.\n",
    "    Truncates long sequences to fit within the model's token limit.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    for path in examples['image_path']:\n",
    "        try:\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {path}: {e}\")\n",
    "            continue\n",
    "        \n",
    "    # Truncate text that exceeds the token limit and process images\n",
    "    text = [caption for caption in examples['Caption']]\n",
    "    inputs = processor(images=images, text=text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    # Return inputs compatible with the model's forward method\n",
    "    return {\n",
    "        'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask'],\n",
    "        'pixel_values': inputs['pixel_values']\n",
    "    }\n",
    "\n",
    "class BLIPFineTuner:\n",
    "    def __init__(self, model_name=\"Salesforce/blip-image-captioning-large\", output_dir=\"./results\"):\n",
    "        \"\"\"\n",
    "        Initializes the BLIP model and processor.\n",
    "        :param model_name: The pre-trained model to fine-tune.\n",
    "        :param output_dir: Directory to save the fine-tuned model.\n",
    "        \"\"\"\n",
    "        self.processor = BlipProcessor.from_pretrained(model_name)\n",
    "        self.output_dir = output_dir\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "\n",
    "    def process_in_chunks(self, dataset, chunk_size, dataset_type):\n",
    "        \"\"\"\n",
    "        Process the dataset in chunks to avoid memory exhaustion.\n",
    "        :param dataset: The full dataset to process.\n",
    "        :param chunk_size: Number of samples to process at once.\n",
    "        :param dataset_type: Type of dataset being processed (\"train\" or \"validation\").\n",
    "        \"\"\"\n",
    "        # Check if processed chunks already exist\n",
    "        processed_chunk_dir = f\"./processed_{dataset_type}_chunks/\"\n",
    "        if os.path.exists(processed_chunk_dir):\n",
    "            print(f\"Loading preprocessed {dataset_type} chunks from disk...\")\n",
    "            processed_dataset = load_from_disk(processed_chunk_dir)\n",
    "            return processed_dataset\n",
    "\n",
    "        print(f\"No preprocessed {dataset_type} data found. Processing now...\")\n",
    "        # Process the dataset in chunks if no cached version is found\n",
    "        processed_chunks = []\n",
    "        for start_idx in range(0, len(dataset), chunk_size):\n",
    "            end_idx = min(start_idx + chunk_size, len(dataset))\n",
    "            chunk = dataset.select(range(start_idx, end_idx))\n",
    "            print(f\"Processing {dataset_type} chunk from {start_idx} to {end_idx}...\")\n",
    "            processed_chunk = chunk.map(lambda examples: preprocess_data(examples, self.processor), \n",
    "                                        batched=True, batch_size=1, load_from_cache_file=False)\n",
    "            \n",
    "            # Save processed chunk to memory and disk\n",
    "            print(f\"Saving {dataset_type} processed chunk from {start_idx} to {end_idx}...\")\n",
    "            processed_chunks.append(processed_chunk)\n",
    "\n",
    "        # Concatenate all processed chunks into one dataset and save\n",
    "        processed_dataset = Dataset.concatenate_datasets(processed_chunks)\n",
    "        processed_dataset.save_to_disk(processed_chunk_dir)\n",
    "        return processed_dataset\n",
    "\n",
    "    def fine_tune(self, dataset, num_train_epochs=3, batch_size=4, chunk_size=1000):\n",
    "        \"\"\"\n",
    "        Fine-tunes the BLIP model on the provided dataset.\n",
    "        :param dataset: The dataset containing training and validation data.\n",
    "        :param num_train_epochs: Number of epochs for fine-tuning.\n",
    "        :param batch_size: Batch size for training and evaluation.\n",
    "        :param chunk_size: Size of dataset chunks to process.\n",
    "        \"\"\"\n",
    "        # Load the model if it's not already loaded\n",
    "        if self.model is None:\n",
    "            print(f\"Loading model {self.model_name}...\")\n",
    "            self.model = BlipForConditionalGeneration.from_pretrained(self.model_name)\n",
    "\n",
    "        # Move model to GPU if available\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {device}\")\n",
    "        self.model.to(device)\n",
    "\n",
    "        # Process or load the dataset in chunks to prevent memory overflow\n",
    "        print(\"Processing or loading training set in chunks...\")\n",
    "        processed_train_dataset = self.process_in_chunks(dataset[\"train\"], chunk_size, dataset_type=\"train\")\n",
    "        print(\"Processing or loading validation set in chunks...\")\n",
    "        processed_validation_dataset = self.process_in_chunks(dataset[\"validation\"], chunk_size, dataset_type=\"validation\")\n",
    "\n",
    "        # Define training arguments with caching\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            save_steps=1000,\n",
    "            save_total_limit=2,\n",
    "            logging_dir=\"./logs\",\n",
    "            learning_rate=5e-5,\n",
    "            logging_steps=100,  # Log and evaluate every 100 steps\n",
    "            eval_steps=500,  # Evaluate the model every 500 steps\n",
    "            remove_unused_columns=False,  # Keep extra columns like ID, image_path, etc.\n",
    "            load_best_model_at_end=True  # Load the best model checkpoint at the end of training\n",
    "        )\n",
    "\n",
    "        # Define the Trainer\n",
    "        print(\"Initializing Trainer...\")\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=processed_train_dataset,\n",
    "            eval_dataset=processed_validation_dataset,\n",
    "        )\n",
    "\n",
    "        # Start training\n",
    "        print(\"Starting training...\")\n",
    "        trainer.train()\n",
    "\n",
    "        # Save the fine-tuned model\n",
    "        self.save_model()\n",
    "        print(f\"Model saved at {self.output_dir}\")\n",
    "\n",
    "    def save_model(self):\n",
    "        \"\"\"\n",
    "        Saves the fine-tuned model.\n",
    "        \"\"\"\n",
    "        if self.model:\n",
    "            print(\"Saving model...\")\n",
    "            self.model.save_pretrained(self.output_dir)\n",
    "            self.processor.save_pretrained(self.output_dir)\n",
    "        else:\n",
    "            print(\"Model is not loaded, cannot save.\")\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"\n",
    "        Loads the saved fine-tuned model from disk.\n",
    "        \"\"\"\n",
    "        if os.path.exists(self.output_dir):\n",
    "            print(f\"Loading model from {self.output_dir}...\")\n",
    "            self.model = BlipForConditionalGeneration.from_pretrained(self.output_dir)\n",
    "            self.processor = BlipProcessor.from_pretrained(self.output_dir)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No model found at {self.output_dir}\")\n",
    "\n",
    "    def generate_caption(self, image_path):\n",
    "        \"\"\"\n",
    "        Generates a caption for a given image using the fine-tuned model.\n",
    "        :param image_path: Path to the medical image.\n",
    "        :return: Generated caption.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"The model is not loaded. Call `load_model()` or train the model first.\")\n",
    "\n",
    "        print(f\"Generating caption for image: {image_path}\")\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        out = self.model.generate(**inputs)\n",
    "        generated_caption = self.processor.decode(out[0], skip_special_tokens=True)\n",
    "        return generated_caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e12624e-e782-49e4-aafb-38559e7b2114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda3\\envs\\MEDCAT\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning...\n",
      "Loading model Salesforce/blip-image-captioning-large...\n",
      "Using device: cuda\n",
      "Processing or loading training set in chunks...\n",
      "No preprocessed train data found. Processing now...\n",
      "Processing train chunk from 0 to 3000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28075a486b544a189b79844869316348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving train processed chunk from 0 to 3000...\n",
      "Processing train chunk from 3000 to 6000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4b4c32e77c94f26a7fb5b3528ad1903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving train processed chunk from 3000 to 6000...\n",
      "Processing train chunk from 6000 to 9000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a9ea520264b4db4b091ab72bd8cb720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving train processed chunk from 6000 to 9000...\n",
      "Processing train chunk from 9000 to 12000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "530fc761e7644a628248dabe217ea2c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving train processed chunk from 9000 to 12000...\n",
      "Processing train chunk from 12000 to 15000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4eebd4c39334d2e996ba76da562fa04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving train processed chunk from 12000 to 15000...\n",
      "Processing train chunk from 15000 to 18000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<bound method BLIPFineTuner.preprocess_data of <__main__.BLIPFineTuner object at 0x000002588AB7A7D0>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f31b3b9cc786446e8bce19e1579ef2da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ArrowMemoryError",
     "evalue": "realloc of size 4294967296 failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 31\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Caption: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaption\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 31\u001b[0m     main(load_existing_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[4], line 22\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(load_existing_model)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Fine-tune the model\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting fine-tuning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m     fine_tuner\u001b[38;5;241m.\u001b[39mfine_tune(dataset, num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3000\u001b[39m)  \u001b[38;5;66;03m# Process in chunks\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Perform inference on a new image\u001b[39;00m\n\u001b[0;32m     25\u001b[0m test_image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/path_to_new_image.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with the path to a new medical image\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 101\u001b[0m, in \u001b[0;36mBLIPFineTuner.fine_tune\u001b[1;34m(self, dataset, num_train_epochs, batch_size, chunk_size)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Process or load the dataset in chunks to prevent memory overflow\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing or loading training set in chunks...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 101\u001b[0m processed_train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_in_chunks(dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m], chunk_size, dataset_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing or loading validation set in chunks...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    103\u001b[0m processed_validation_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_in_chunks(dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m], chunk_size, dataset_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 70\u001b[0m, in \u001b[0;36mBLIPFineTuner.process_in_chunks\u001b[1;34m(self, dataset, chunk_size, dataset_type)\u001b[0m\n\u001b[0;32m     68\u001b[0m chunk \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(start_idx, end_idx))\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m chunk from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 70\u001b[0m processed_chunk \u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_data, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, load_from_cache_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Save processed chunk to memory and disk\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m processed chunk from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mG:\\anaconda3\\envs\\MEDCAT\\Lib\\site-packages\\datasets\\arrow_dataset.py:602\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    601\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 602\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    603\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32mG:\\anaconda3\\envs\\MEDCAT\\Lib\\site-packages\\datasets\\arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    565\u001b[0m }\n\u001b[0;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mG:\\anaconda3\\envs\\MEDCAT\\Lib\\site-packages\\datasets\\arrow_dataset.py:3167\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3162\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3163\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3164\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3165\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3166\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3167\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[0;32m   3168\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m   3169\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mG:\\anaconda3\\envs\\MEDCAT\\Lib\\site-packages\\datasets\\arrow_dataset.py:3581\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3579\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwrite_table(batch\u001b[38;5;241m.\u001b[39mto_arrow())\n\u001b[0;32m   3580\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3581\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwrite_batch(batch)\n\u001b[0;32m   3582\u001b[0m num_examples_progress_update \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_examples_in_batch\n\u001b[0;32m   3583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m _time \u001b[38;5;241m+\u001b[39m config\u001b[38;5;241m.\u001b[39mPBAR_REFRESH_TIME_INTERVAL:\n",
      "File \u001b[1;32mG:\\anaconda3\\envs\\MEDCAT\\Lib\\site-packages\\datasets\\arrow_writer.py:572\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[1;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[0;32m    570\u001b[0m schema \u001b[38;5;241m=\u001b[39m inferred_features\u001b[38;5;241m.\u001b[39marrow_schema \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpa_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema\n\u001b[0;32m    571\u001b[0m pa_table \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_arrays(arrays, schema\u001b[38;5;241m=\u001b[39mschema)\n\u001b[1;32m--> 572\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_table(pa_table, writer_batch_size)\n",
      "File \u001b[1;32mG:\\anaconda3\\envs\\MEDCAT\\Lib\\site-packages\\datasets\\arrow_writer.py:590\u001b[0m, in \u001b[0;36mArrowWriter.write_table\u001b[1;34m(self, pa_table, writer_batch_size)\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_bytes \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mnbytes\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_examples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mnum_rows\n\u001b[1;32m--> 590\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpa_writer\u001b[38;5;241m.\u001b[39mwrite_table(pa_table, writer_batch_size)\n",
      "File \u001b[1;32mG:\\anaconda3\\envs\\MEDCAT\\Lib\\site-packages\\pyarrow\\ipc.pxi:529\u001b[0m, in \u001b[0;36mpyarrow.lib._CRecordBatchWriter.write_table\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mG:\\anaconda3\\envs\\MEDCAT\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowMemoryError\u001b[0m: realloc of size 4294967296 failed"
     ]
    }
   ],
   "source": [
    "def main(load_existing_model=False):\n",
    "    # Paths to your data\n",
    "    image_dir_train = \"../Datasets/ROCO2/train_images/train\"   \n",
    "    image_dir_val = \"../Datasets/ROCO2/valid_images/valid\"\n",
    "    annotation_file_train = \"../Datasets/ROCO2/train_captions.csv\"  \n",
    "    annotation_file_val =  \"../Datasets/ROCO2/valid_captions.csv\"  \n",
    "\n",
    "    # Load the dataset\n",
    "    data_loader = ROCODataLoader(image_dir_train, image_dir_val, annotation_file_train, annotation_file_val)\n",
    "    dataset = data_loader.load_data()\n",
    "\n",
    "    # Initialize the BLIP fine-tuner\n",
    "    fine_tuner = BLIPFineTuner()\n",
    "\n",
    "    if load_existing_model:\n",
    "        # Load the saved model\n",
    "        print(\"Loading the existing model...\")\n",
    "        fine_tuner.load_model()\n",
    "    else:\n",
    "        # Fine-tune the model\n",
    "        print(\"Starting fine-tuning...\")\n",
    "        fine_tuner.fine_tune(dataset, num_train_epochs=3, batch_size=4, chunk_size=1000)\n",
    "\n",
    "    # Perform inference on a new image\n",
    "    test_image_path = \"/path_to_new_image.jpg\"  # Replace with the path to a new medical image\n",
    "    caption = fine_tuner.generate_caption(test_image_path)\n",
    "    print(f\"Generated Caption: {caption}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(load_existing_model=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
