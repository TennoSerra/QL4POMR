{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9be4940b-b967-4b4a-a075-82f78a070345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import Dataset, DatasetDict, load_from_disk, concatenate_datasets\n",
    "from PIL import Image\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51da99bf-d088-4375-8c04-3cdb2c255fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROCODataLoader:\n",
    "    def __init__(self, image_dir_train, image_dir_val, annotation_file_train, annotation_file_val):\n",
    "        \"\"\"\n",
    "        Initializes the data loader with directories for training and validation images, and annotation files.\n",
    "        :param image_dir_train: Directory where training images are stored.\n",
    "        :param image_dir_val: Directory where validation images are stored.\n",
    "        :param annotation_file_train: Path to the CSV file for training annotations.\n",
    "        :param annotation_file_val: Path to the CSV file for validation annotations.\n",
    "        \"\"\"\n",
    "        self.image_dir_train = image_dir_train\n",
    "        self.image_dir_val = image_dir_val\n",
    "        self.annotation_file_train = annotation_file_train\n",
    "        self.annotation_file_val = annotation_file_val\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Loads and prepares both the training and validation datasets.\n",
    "        :return: A DatasetDict with 'train' and 'validation' datasets.\n",
    "        \"\"\"\n",
    "        train_df = pd.read_csv(self.annotation_file_train)\n",
    "        val_df = pd.read_csv(self.annotation_file_val)\n",
    "\n",
    "        # Add full image paths for training and validation sets\n",
    "        train_df['image_path'] = train_df['ID'].apply(lambda x: os.path.join(self.image_dir_train, x + \".jpg\"))\n",
    "        val_df['image_path'] = val_df['ID'].apply(lambda x: os.path.join(self.image_dir_val, x + \".jpg\"))\n",
    "\n",
    "        # Convert to Hugging Face dataset format\n",
    "        train_dataset = Dataset.from_pandas(train_df)\n",
    "        val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "        # Return a DatasetDict containing both training and validation sets\n",
    "        return DatasetDict({'train': train_dataset, 'validation': val_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41c88049-b68f-4f77-91b4-14a1b31d7162",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLIPDataCollator:\n",
    "    def __init__(self, processor, padding=True):\n",
    "        self.processor = processor\n",
    "        self.padding = padding\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # Extract text inputs (input_ids) and pixel values (images) separately\n",
    "        text_inputs = [feature['input_ids'] for feature in features]\n",
    "        labels = [feature['input_ids'] for feature in features]  # Use input_ids as labels\n",
    "        \n",
    "        # Ensure pixel_values are tensors before stacking\n",
    "        pixel_values = [torch.tensor(feature['pixel_values']) if not isinstance(feature['pixel_values'], torch.Tensor) \n",
    "                        else feature['pixel_values'] for feature in features]\n",
    "\n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "\n",
    "        # Pad the text inputs and labels\n",
    "        text_inputs = self.processor.tokenizer.pad(\n",
    "            {'input_ids': text_inputs},\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        labels = self.processor.tokenizer.pad(\n",
    "            {'input_ids': labels},\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Return the padded text, the pixel values, and the labels as a dictionary\n",
    "        return {\n",
    "            'input_ids': text_inputs['input_ids'],\n",
    "            'attention_mask': text_inputs['attention_mask'],\n",
    "            'pixel_values': pixel_values,\n",
    "            'labels': labels['input_ids']  # Add labels for loss computation\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53b49e8e-3698-44fb-be82-e6c1c0f38ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples, processor):\n",
    "    # Define the transformation pipeline (resize and convert to tensor)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize to 224x224\n",
    "        transforms.ToTensor(),  # Convert to tensor, normalizes pixel values between 0 and 1\n",
    "    ])\n",
    "\n",
    "    # Apply transformations to the images\n",
    "    images = []\n",
    "    for path in examples['image_path']:\n",
    "        try:\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "            img = transform(img)  # Apply transformation\n",
    "            images.append(img)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Stack the images into a batch and keep them on CPU initially\n",
    "    images = torch.stack(images)\n",
    "\n",
    "    # Process images separately using the processor, and set `do_rescale=False`\n",
    "    image_inputs = processor(images=images, return_tensors=\"pt\", do_rescale=False)  # Disable rescaling\n",
    "\n",
    "    # Tokenize text\n",
    "    text = [caption for caption in examples['Caption']]\n",
    "    text_inputs = processor.tokenizer(text=text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    # Move the input tensors to GPU only after pinning (CPU tensors pinned first, then moved to GPU)\n",
    "    return {\n",
    "        'input_ids': text_inputs['input_ids'].to(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        'attention_mask': text_inputs['attention_mask'].to(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        'pixel_values': image_inputs['pixel_values'].to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "514d9aa2-5727-484f-807d-f02f36565190",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLIPFineTuner:\n",
    "    def __init__(self, model_name=\"Salesforce/blip-image-captioning-base\", output_dir=\"./results\"):\n",
    "        self.processor = BlipProcessor.from_pretrained(model_name)\n",
    "        self.output_dir = output_dir\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.checkpoint_dir = \"./checkpoints\"\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    def save_checkpoint(self, epoch, optimizer, scheduler):\n",
    "        \"\"\"Save model and optimizer state after each epoch.\"\"\"\n",
    "        checkpoint_path = os.path.join(self.checkpoint_dir, f\"checkpoint_epoch_{epoch}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict() if scheduler else None\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "    def load_checkpoint(self, optimizer=None, scheduler=None):\n",
    "        \"\"\"Load model from the latest checkpoint.\"\"\"\n",
    "        checkpoints = [f for f in os.listdir(self.checkpoint_dir) if f.startswith(\"checkpoint\")]\n",
    "        if checkpoints:\n",
    "            latest_checkpoint = max(checkpoints, key=lambda x: int(x.split(\"_\")[2].split(\".\")[0]))\n",
    "            checkpoint_path = os.path.join(self.checkpoint_dir, latest_checkpoint)\n",
    "            print(f\"Loading checkpoint {checkpoint_path}...\")\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            if optimizer:\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            if scheduler and 'scheduler_state_dict' in checkpoint:\n",
    "                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            start_epoch = checkpoint['epoch'] + 1\n",
    "            print(f\"Resuming from epoch {start_epoch}\")\n",
    "            return start_epoch\n",
    "        return 0\n",
    "\n",
    "    def process_in_chunks(self, dataset, chunk_size, dataset_type):\n",
    "        processed_chunk_dir = f\"./processed_{dataset_type}_chunks/\"\n",
    "        if os.path.exists(processed_chunk_dir):\n",
    "            print(f\"Loading preprocessed {dataset_type} chunks from disk...\")\n",
    "\n",
    "            # Gather all chunk folders and load each of them\n",
    "            chunk_folders = [os.path.join(processed_chunk_dir, folder) for folder in os.listdir(processed_chunk_dir)]\n",
    "            chunk_datasets = []\n",
    "\n",
    "            for folder in chunk_folders:\n",
    "                try:\n",
    "                    chunk_datasets.append(load_from_disk(folder))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading chunk from {folder}: {e}\")\n",
    "            \n",
    "            # Concatenate all chunk datasets into a single dataset\n",
    "            print(f\"Concatenating {len(chunk_datasets)} chunks into a single dataset...\")\n",
    "            if len(chunk_datasets) > 0:\n",
    "                full_dataset = concatenate_datasets(chunk_datasets)\n",
    "                print(f\"Successfully loaded and concatenated {dataset_type} dataset.\")\n",
    "                return full_dataset\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"No valid chunks found in {processed_chunk_dir}\")\n",
    "        else:\n",
    "            print(f\"No preprocessed {dataset_type} data found. Processing now...\")\n",
    "\n",
    "            # Create a directory for processed chunks if it doesn't exist\n",
    "            os.makedirs(processed_chunk_dir, exist_ok=True)\n",
    "\n",
    "            # Process and save each chunk directly to disk to avoid memory exhaustion\n",
    "            for start_idx in range(0, len(dataset), chunk_size):\n",
    "                end_idx = min(start_idx + chunk_size, len(dataset))\n",
    "                chunk = dataset.select(range(start_idx, end_idx))\n",
    "                print(f\"Processing {dataset_type} chunk from {start_idx} to {end_idx}...\")\n",
    "                processed_chunk = chunk.map(lambda examples: preprocess_data(examples, self.processor), \n",
    "                                            batched=True, batch_size=1, load_from_cache_file=False)\n",
    "\n",
    "                # Save the chunk immediately to disk using save_to_disk()\n",
    "                chunk_path = f\"{processed_chunk_dir}/chunk_{start_idx}_{end_idx}\"\n",
    "                print(f\"Saving {dataset_type} processed chunk from {start_idx} to {end_idx} at {chunk_path}...\")\n",
    "                processed_chunk.save_to_disk(chunk_path)\n",
    "\n",
    "            # Reload the chunks after processing\n",
    "            return self.process_in_chunks(dataset, chunk_size, dataset_type)\n",
    "\n",
    "    def fine_tune(self, dataset, num_train_epochs=3, batch_size=4, chunk_size=1000):\n",
    "        if self.model is None:\n",
    "            print(f\"Loading model {self.model_name}...\")\n",
    "            self.model = BlipForConditionalGeneration.from_pretrained(self.model_name)\n",
    "    \n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {device}\")\n",
    "        self.model.to(device)\n",
    "    \n",
    "        # Optimizer and scheduler setup\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=5e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "    \n",
    "        # Load checkpoint if exists\n",
    "        start_epoch = self.load_checkpoint(optimizer=optimizer, scheduler=scheduler)\n",
    "    \n",
    "        # Process or load the dataset in chunks to prevent memory overflow\n",
    "        print(\"Processing or loading training set in chunks...\")\n",
    "        processed_train_dataset = self.process_in_chunks(dataset[\"train\"], chunk_size, dataset_type=\"train\")\n",
    "        print(\"Processing or loading validation set in chunks...\")\n",
    "        processed_validation_dataset = self.process_in_chunks(dataset[\"validation\"], chunk_size, dataset_type=\"validation\")\n",
    "    \n",
    "        # Ensure data is on the correct device\n",
    "        processed_train_dataset.set_format(\"torch\", device=device)\n",
    "        processed_validation_dataset.set_format(\"torch\", device=device)\n",
    "    \n",
    "        # Use the custom BLIP data collator to handle padding\n",
    "        data_collator = BLIPDataCollator(self.processor)\n",
    "    \n",
    "        # Define training arguments with caching\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            save_steps=1000,\n",
    "            save_total_limit=2,\n",
    "            logging_dir=\"./logs\",\n",
    "            learning_rate=5e-5,\n",
    "            logging_steps=500,\n",
    "            eval_steps=1000,\n",
    "            remove_unused_columns=False,\n",
    "            load_best_model_at_end=True,\n",
    "            gradient_accumulation_steps=8,\n",
    "            fp16=True,  # Enable mixed precision\n",
    "            dataloader_num_workers=0,  # Parallel data loading\n",
    "            report_to=\"none\"\n",
    "        )\n",
    "    \n",
    "        # Define the Trainer\n",
    "        print(\"Initializing Trainer...\")\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=processed_train_dataset,\n",
    "            eval_dataset=processed_validation_dataset,\n",
    "            data_collator=data_collator,\n",
    "            optimizers=(optimizer, scheduler),\n",
    "        )\n",
    "    \n",
    "        # Start training\n",
    "        print(f\"Starting training from epoch {start_epoch}/{num_train_epochs}\")\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "        for epoch in range(start_epoch, num_train_epochs):\n",
    "            print(f\"Training epoch {epoch}/{num_train_epochs}\")\n",
    "            trainer.train()\n",
    "    \n",
    "            # Save checkpoint after each epoch\n",
    "            self.save_checkpoint(epoch, optimizer, scheduler)\n",
    "            print(f\"Checkpoint saved for epoch {epoch}\")\n",
    "    \n",
    "        # Save the final fine-tuned model\n",
    "        self.save_model()\n",
    "        print(f\"Model saved at {self.output_dir}\")\n",
    "    \n",
    "    \n",
    "        def save_model(self):\n",
    "            if self.model:\n",
    "                print(\"Saving model...\")\n",
    "                self.model.save_pretrained(self.output_dir)\n",
    "                self.processor.save_pretrained(self.output_dir)\n",
    "            else:\n",
    "                print(\"Model is not loaded, cannot save.\")\n",
    "    \n",
    "        def load_model(self):\n",
    "            \"\"\"Load the final fine-tuned model if it exists.\"\"\"\n",
    "            if os.path.exists(self.output_dir):\n",
    "                print(f\"Loading model from {self.output_dir}...\")\n",
    "                self.model = BlipForConditionalGeneration.from_pretrained(self.output_dir)\n",
    "                self.processor = BlipProcessor.from_pretrained(self.output_dir)\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"No model found at {self.output_dir}\")\n",
    "    \n",
    "        def generate_caption(self, image_path):\n",
    "            if self.model is None:\n",
    "                raise ValueError(\"The model is not loaded. Call `load_model()` or train the model first.\")\n",
    "    \n",
    "            print(f\"Generating caption for image: {image_path}\")\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            inputs = self.processor(images=image, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            out = self.model.generate(**inputs)\n",
    "            generated_caption = self.processor.decode(out[0], skip_special_tokens=True)\n",
    "            return generated_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e12624e-e782-49e4-aafb-38559e7b2114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda3\\envs\\MEDCAT\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning...\n",
      "Loading model Salesforce/blip-image-captioning-base...\n",
      "Using device: cuda\n",
      "Processing or loading training set in chunks...\n",
      "Loading preprocessed train chunks from disk...\n",
      "Concatenating 21 chunks into a single dataset...\n",
      "Successfully loaded and concatenated train dataset.\n",
      "Processing or loading validation set in chunks...\n",
      "Loading preprocessed validation chunks from disk...\n",
      "Concatenating 4 chunks into a single dataset...\n",
      "Successfully loaded and concatenated validation dataset.\n",
      "Initializing Trainer...\n",
      "Starting training from epoch 0/3\n",
      "Training epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda3\\envs\\MEDCAT\\Lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "G:\\anaconda3\\envs\\MEDCAT\\Lib\\site-packages\\accelerate\\accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 31\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Caption: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaption\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 31\u001b[0m     main(load_existing_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[11], line 22\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(load_existing_model)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Fine-tune the model\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting fine-tuning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m     fine_tuner\u001b[38;5;241m.\u001b[39mfine_tune(dataset, num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3000\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Perform inference on a new image\u001b[39;00m\n\u001b[0;32m     25\u001b[0m test_image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample1.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with the path to a new medical image\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 151\u001b[0m, in \u001b[0;36mBLIPFineTuner.fine_tune\u001b[1;34m(self, dataset, num_train_epochs, batch_size, chunk_size)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, num_train_epochs):\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_train_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 151\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# Save checkpoint after each epoch\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_checkpoint(epoch, optimizer, scheduler)\n",
      "File \u001b[1;32mG:\\anaconda3\\envs\\MEDCAT\\Lib\\site-packages\\transformers\\trainer.py:1948\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1946\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1948\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1949\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   1950\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   1951\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   1952\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   1953\u001b[0m     )\n",
      "File \u001b[1;32mG:\\anaconda3\\envs\\MEDCAT\\Lib\\site-packages\\transformers\\trainer.py:2246\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2243\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2245\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 2246\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   2247\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n",
      "File \u001b[1;32mG:\\anaconda3\\envs\\MEDCAT\\Lib\\site-packages\\accelerate\\data_loader.py:454\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 454\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[1;32mG:\\anaconda3\\envs\\MEDCAT\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mG:\\anaconda3\\envs\\MEDCAT\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mG:\\anaconda3\\envs\\MEDCAT\\Lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py:69\u001b[0m, in \u001b[0;36mpin_memory\u001b[1;34m(data, device)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# The sequence type may have extra properties, so we can't just\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new sequence.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# Create a clone and update it if the sequence type is mutable.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(data)\n\u001b[1;32m---> 69\u001b[0m     clone\u001b[38;5;241m.\u001b[39mupdate({k: pin_memory(sample, device) \u001b[38;5;28;01mfor\u001b[39;00m k, sample \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems()})\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mG:\\anaconda3\\envs\\MEDCAT\\Lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py:69\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# The sequence type may have extra properties, so we can't just\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new sequence.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# Create a clone and update it if the sequence type is mutable.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(data)\n\u001b[1;32m---> 69\u001b[0m     clone\u001b[38;5;241m.\u001b[39mupdate({k: pin_memory(sample, device) \u001b[38;5;28;01mfor\u001b[39;00m k, sample \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems()})\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mG:\\anaconda3\\envs\\MEDCAT\\Lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py:59\u001b[0m, in \u001b[0;36mpin_memory\u001b[1;34m(data, device)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpin_memory\u001b[39m(data, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m---> 59\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m data\u001b[38;5;241m.\u001b[39mpin_memory(device)\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m)):\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned"
     ]
    }
   ],
   "source": [
    "def main(load_existing_model=False):\n",
    "    # Paths to your data\n",
    "    image_dir_train = \"../Datasets/ROCO2/train_images/train\"   \n",
    "    image_dir_val = \"../Datasets/ROCO2/valid_images/valid\"\n",
    "    annotation_file_train = \"../Datasets/ROCO2/train_captions.csv\"  \n",
    "    annotation_file_val =  \"../Datasets/ROCO2/valid_captions.csv\"  \n",
    "\n",
    "    # Load the dataset\n",
    "    data_loader = ROCODataLoader(image_dir_train, image_dir_val, annotation_file_train, annotation_file_val)\n",
    "    dataset = data_loader.load_data()\n",
    "\n",
    "    # Initialize the BLIP fine-tuner\n",
    "    fine_tuner = BLIPFineTuner()\n",
    "\n",
    "    if load_existing_model:\n",
    "        # Load the saved model\n",
    "        print(\"Loading the existing model...\")\n",
    "        fine_tuner.load_model()\n",
    "    else:\n",
    "        # Fine-tune the model\n",
    "        print(\"Starting fine-tuning...\")\n",
    "        fine_tuner.fine_tune(dataset, num_train_epochs=3, batch_size=4, chunk_size=3000)\n",
    "\n",
    "    # Perform inference on a new image\n",
    "    test_image_path = \"sample1.jpg\"  # Replace with the path to a new medical image\n",
    "    caption = fine_tuner.generate_caption(test_image_path)\n",
    "    print(f\"Generated Caption: {caption}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(load_existing_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c809cb8-f437-4d14-9a93-949a90d9e5b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
