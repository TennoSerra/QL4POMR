{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9be4940b-b967-4b4a-a075-82f78a070345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import Dataset, DatasetDict, load_from_disk, concatenate_datasets\n",
    "from PIL import Image\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51da99bf-d088-4375-8c04-3cdb2c255fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROCODataLoader:\n",
    "    def __init__(self, image_dir_train, image_dir_val, annotation_file_train, annotation_file_val):\n",
    "        \"\"\"\n",
    "        Initializes the data loader with directories for training and validation images, and annotation files.\n",
    "        :param image_dir_train: Directory where training images are stored.\n",
    "        :param image_dir_val: Directory where validation images are stored.\n",
    "        :param annotation_file_train: Path to the CSV file for training annotations.\n",
    "        :param annotation_file_val: Path to the CSV file for validation annotations.\n",
    "        \"\"\"\n",
    "        self.image_dir_train = image_dir_train\n",
    "        self.image_dir_val = image_dir_val\n",
    "        self.annotation_file_train = annotation_file_train\n",
    "        self.annotation_file_val = annotation_file_val\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Loads and prepares both the training and validation datasets.\n",
    "        :return: A DatasetDict with 'train' and 'validation' datasets.\n",
    "        \"\"\"\n",
    "        train_df = pd.read_csv(self.annotation_file_train)\n",
    "        val_df = pd.read_csv(self.annotation_file_val)\n",
    "\n",
    "        # Add full image paths for training and validation sets\n",
    "        train_df['image_path'] = train_df['ID'].apply(lambda x: os.path.join(self.image_dir_train, x + \".jpg\"))\n",
    "        val_df['image_path'] = val_df['ID'].apply(lambda x: os.path.join(self.image_dir_val, x + \".jpg\"))\n",
    "\n",
    "        # Convert to Hugging Face dataset format\n",
    "        train_dataset = Dataset.from_pandas(train_df)\n",
    "        val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "        # Return a DatasetDict containing both training and validation sets\n",
    "        return DatasetDict({'train': train_dataset, 'validation': val_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41c88049-b68f-4f77-91b4-14a1b31d7162",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLIPDataCollator:\n",
    "    def __init__(self, processor, padding=True):\n",
    "        self.processor = processor\n",
    "        self.padding = padding\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # Extract text inputs (input_ids) and pixel values (images) separately\n",
    "        text_inputs = [feature['input_ids'] for feature in features]\n",
    "        labels = [feature['input_ids'] for feature in features]  # Use input_ids as labels\n",
    "        \n",
    "        # Ensure pixel_values are tensors before stacking\n",
    "        pixel_values = [torch.tensor(feature['pixel_values']) if not isinstance(feature['pixel_values'], torch.Tensor) \n",
    "                        else feature['pixel_values'] for feature in features]\n",
    "\n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "\n",
    "        # Pad the text inputs and labels\n",
    "        text_inputs = self.processor.tokenizer.pad(\n",
    "            {'input_ids': text_inputs},\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        labels = self.processor.tokenizer.pad(\n",
    "            {'input_ids': labels},\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Return the padded text, the pixel values, and the labels as a dictionary\n",
    "        return {\n",
    "            'input_ids': text_inputs['input_ids'],\n",
    "            'attention_mask': text_inputs['attention_mask'],\n",
    "            'pixel_values': pixel_values,\n",
    "            'labels': labels['input_ids']  # Add labels for loss computation\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53b49e8e-3698-44fb-be82-e6c1c0f38ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples, processor):\n",
    "    # Define the transformation pipeline (resize and convert to tensor)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize to 224x224\n",
    "        transforms.ToTensor(),  # Convert to tensor, normalizes pixel values between 0 and 1\n",
    "    ])\n",
    "\n",
    "    # Apply transformations to the images\n",
    "    images = []\n",
    "    for path in examples['image_path']:\n",
    "        try:\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "            img = transform(img)  # Apply transformation\n",
    "            images.append(img)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Stack the images into a batch and keep them on CPU initially\n",
    "    images = torch.stack(images)\n",
    "\n",
    "    # Process images separately using the processor, and set `do_rescale=False`\n",
    "    image_inputs = processor(images=images, return_tensors=\"pt\", do_rescale=False)  # Disable rescaling\n",
    "\n",
    "    # Tokenize text\n",
    "    text = [caption for caption in examples['Caption']]\n",
    "    text_inputs = processor.tokenizer(text=text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    # Move the input tensors to GPU only after pinning (CPU tensors pinned first, then moved to GPU)\n",
    "    return {\n",
    "        'input_ids': text_inputs['input_ids'].to(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        'attention_mask': text_inputs['attention_mask'].to(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        'pixel_values': image_inputs['pixel_values'].to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "514d9aa2-5727-484f-807d-f02f36565190",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLIPFineTuner:\n",
    "    def __init__(self, model_name=\"Salesforce/blip-image-captioning-base\", output_dir=\"./results\"):\n",
    "        self.processor = BlipProcessor.from_pretrained(model_name)\n",
    "        self.output_dir = output_dir\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.checkpoint_dir = \"./checkpoints\"\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    def save_checkpoint(self, epoch, optimizer, scheduler):\n",
    "        \"\"\"Save model and optimizer state after each epoch.\"\"\"\n",
    "        checkpoint_path = os.path.join(self.checkpoint_dir, f\"checkpoint_epoch_{epoch}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict() if scheduler else None\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "    def load_checkpoint(self, optimizer=None, scheduler=None):\n",
    "        \"\"\"Load model from the latest checkpoint.\"\"\"\n",
    "        checkpoints = [f for f in os.listdir(self.checkpoint_dir) if f.startswith(\"checkpoint\")]\n",
    "        if checkpoints:\n",
    "            latest_checkpoint = max(checkpoints, key=lambda x: int(x.split(\"_\")[2].split(\".\")[0]))\n",
    "            checkpoint_path = os.path.join(self.checkpoint_dir, latest_checkpoint)\n",
    "            print(f\"Loading checkpoint {checkpoint_path}...\")\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            if optimizer:\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            if scheduler and 'scheduler_state_dict' in checkpoint:\n",
    "                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            start_epoch = checkpoint['epoch'] + 1\n",
    "            print(f\"Resuming from epoch {start_epoch}\")\n",
    "            return start_epoch\n",
    "        return 0\n",
    "\n",
    "    def process_in_chunks(self, dataset, chunk_size, dataset_type):\n",
    "        processed_chunk_dir = f\"./processed_{dataset_type}_chunks/\"\n",
    "        if os.path.exists(processed_chunk_dir):\n",
    "            print(f\"Loading preprocessed {dataset_type} chunks from disk...\")\n",
    "\n",
    "            # Gather all chunk folders and load each of them\n",
    "            chunk_folders = [os.path.join(processed_chunk_dir, folder) for folder in os.listdir(processed_chunk_dir)]\n",
    "            chunk_datasets = []\n",
    "\n",
    "            for folder in chunk_folders:\n",
    "                try:\n",
    "                    chunk_datasets.append(load_from_disk(folder))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading chunk from {folder}: {e}\")\n",
    "            \n",
    "            # Concatenate all chunk datasets into a single dataset\n",
    "            print(f\"Concatenating {len(chunk_datasets)} chunks into a single dataset...\")\n",
    "            if len(chunk_datasets) > 0:\n",
    "                full_dataset = concatenate_datasets(chunk_datasets)\n",
    "                print(f\"Successfully loaded and concatenated {dataset_type} dataset.\")\n",
    "                return full_dataset\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"No valid chunks found in {processed_chunk_dir}\")\n",
    "        else:\n",
    "            print(f\"No preprocessed {dataset_type} data found. Processing now...\")\n",
    "\n",
    "            # Create a directory for processed chunks if it doesn't exist\n",
    "            os.makedirs(processed_chunk_dir, exist_ok=True)\n",
    "\n",
    "            # Process and save each chunk directly to disk to avoid memory exhaustion\n",
    "            for start_idx in range(0, len(dataset), chunk_size):\n",
    "                end_idx = min(start_idx + chunk_size, len(dataset))\n",
    "                chunk = dataset.select(range(start_idx, end_idx))\n",
    "                print(f\"Processing {dataset_type} chunk from {start_idx} to {end_idx}...\")\n",
    "                processed_chunk = chunk.map(lambda examples: preprocess_data(examples, self.processor), \n",
    "                                            batched=True, batch_size=1, load_from_cache_file=False)\n",
    "\n",
    "                # Save the chunk immediately to disk using save_to_disk()\n",
    "                chunk_path = f\"{processed_chunk_dir}/chunk_{start_idx}_{end_idx}\"\n",
    "                print(f\"Saving {dataset_type} processed chunk from {start_idx} to {end_idx} at {chunk_path}...\")\n",
    "                processed_chunk.save_to_disk(chunk_path)\n",
    "\n",
    "            # Reload the chunks after processing\n",
    "            return self.process_in_chunks(dataset, chunk_size, dataset_type)\n",
    "\n",
    "    def fine_tune(self, dataset, num_train_epochs=3, batch_size=4, chunk_size=3000):\n",
    "        if self.model is None:\n",
    "            print(f\"Loading model {self.model_name}...\")\n",
    "            self.model = BlipForConditionalGeneration.from_pretrained(self.model_name)\n",
    "\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {device}\")\n",
    "        self.model.to(device)\n",
    "\n",
    "        # Optimizer and scheduler setup\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=5e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "        # Load checkpoint if exists\n",
    "        start_epoch = self.load_checkpoint(optimizer=optimizer, scheduler=scheduler)\n",
    "\n",
    "        # Process or load the dataset in chunks to prevent memory overflow\n",
    "        print(\"Processing or loading training set in chunks...\")\n",
    "        processed_train_dataset = self.process_in_chunks(dataset[\"train\"], chunk_size, dataset_type=\"train\")\n",
    "        print(\"Processing or loading validation set in chunks...\")\n",
    "        processed_validation_dataset = self.process_in_chunks(dataset[\"validation\"], chunk_size, dataset_type=\"validation\")\n",
    "\n",
    "        # Use the custom BLIP data collator to handle padding\n",
    "        data_collator = BLIPDataCollator(self.processor)\n",
    "\n",
    "        # Set up DataLoaders manually to ensure pin_memory is disabled\n",
    "        train_dataloader = DataLoader(\n",
    "            processed_train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=False  # Disable pinning to avoid issues\n",
    "        )\n",
    "\n",
    "        validation_dataloader = DataLoader(\n",
    "            processed_validation_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=False  # Disable pinning\n",
    "        )\n",
    "\n",
    "        # Define training arguments with caching\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            eval_strategy=\"steps\",\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            save_steps=1000,\n",
    "            save_total_limit=2,\n",
    "            logging_dir=\"./logs\",\n",
    "            learning_rate=5e-5,\n",
    "            logging_steps=500,\n",
    "            eval_steps=1000,\n",
    "            remove_unused_columns=False,\n",
    "            load_best_model_at_end=True,\n",
    "            gradient_accumulation_steps=8,\n",
    "            fp16=True,  # Enable mixed precision\n",
    "            dataloader_num_workers=0,  # Disable parallel loading\n",
    "            report_to=\"none\"  # Disable W&B or other reporting\n",
    "        )\n",
    "\n",
    "        # Initialize Trainer with manually defined DataLoaders\n",
    "        print(\"Initializing Trainer...\")\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=processed_train_dataset,\n",
    "            eval_dataset=processed_validation_dataset,\n",
    "            data_collator=data_collator,\n",
    "            optimizers=(optimizer, scheduler),\n",
    "        )\n",
    "\n",
    "        # Start training\n",
    "        print(f\"Starting training from epoch {start_epoch}/{num_train_epochs}\")\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        for epoch in range(start_epoch, num_train_epochs):\n",
    "            print(f\"Training epoch {epoch}/{num_train_epochs}\")\n",
    "            trainer.train()\n",
    "\n",
    "            # Save checkpoint after each epoch\n",
    "            self.save_checkpoint(epoch, optimizer, scheduler)\n",
    "            print(f\"Checkpoint saved for epoch {epoch}\")\n",
    "\n",
    "        # Save the final fine-tuned model\n",
    "        self.save_model()\n",
    "        print(f\"Model saved at {self.output_dir}\")\n",
    "    \n",
    "    \n",
    "        def save_model(self):\n",
    "            if self.model:\n",
    "                print(\"Saving model...\")\n",
    "                self.model.save_pretrained(self.output_dir)\n",
    "                self.processor.save_pretrained(self.output_dir)\n",
    "            else:\n",
    "                print(\"Model is not loaded, cannot save.\")\n",
    "    \n",
    "        def load_model(self):\n",
    "            \"\"\"Load the final fine-tuned model if it exists.\"\"\"\n",
    "            if os.path.exists(self.output_dir):\n",
    "                print(f\"Loading model from {self.output_dir}...\")\n",
    "                self.model = BlipForConditionalGeneration.from_pretrained(self.output_dir)\n",
    "                self.processor = BlipProcessor.from_pretrained(self.output_dir)\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"No model found at {self.output_dir}\")\n",
    "    \n",
    "        def generate_caption(self, image_path):\n",
    "            if self.model is None:\n",
    "                raise ValueError(\"The model is not loaded. Call `load_model()` or train the model first.\")\n",
    "    \n",
    "            print(f\"Generating caption for image: {image_path}\")\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            inputs = self.processor(images=image, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            out = self.model.generate(**inputs)\n",
    "            generated_caption = self.processor.decode(out[0], skip_special_tokens=True)\n",
    "            return generated_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e12624e-e782-49e4-aafb-38559e7b2114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda3\\envs\\MEDCAT\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning...\n",
      "Loading model Salesforce/blip-image-captioning-base...\n",
      "Using device: cuda\n",
      "Processing or loading training set in chunks...\n",
      "Loading preprocessed train chunks from disk...\n",
      "Concatenating 21 chunks into a single dataset...\n",
      "Successfully loaded and concatenated train dataset.\n",
      "Processing or loading validation set in chunks...\n",
      "Loading preprocessed validation chunks from disk...\n",
      "Concatenating 4 chunks into a single dataset...\n",
      "Successfully loaded and concatenated validation dataset.\n",
      "Initializing Trainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda3\\envs\\MEDCAT\\Lib\\site-packages\\accelerate\\accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from epoch 0/3\n",
      "Training epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='284' max='11280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  284/11280 33:41 < 21:53:28, 0.14 it/s, Epoch 0.08/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main(load_existing_model=False):\n",
    "    # Paths to your data\n",
    "    image_dir_train = \"../Datasets/ROCO2/train_images/train\"   \n",
    "    image_dir_val = \"../Datasets/ROCO2/valid_images/valid\"\n",
    "    annotation_file_train = \"../Datasets/ROCO2/train_captions.csv\"  \n",
    "    annotation_file_val =  \"../Datasets/ROCO2/valid_captions.csv\"  \n",
    "\n",
    "    # Load the dataset\n",
    "    data_loader = ROCODataLoader(image_dir_train, image_dir_val, annotation_file_train, annotation_file_val)\n",
    "    dataset = data_loader.load_data()\n",
    "\n",
    "    # Initialize the BLIP fine-tuner\n",
    "    fine_tuner = BLIPFineTuner()\n",
    "\n",
    "    if load_existing_model:\n",
    "        # Load the saved model\n",
    "        print(\"Loading the existing model...\")\n",
    "        fine_tuner.load_model()\n",
    "    else:\n",
    "        # Fine-tune the model\n",
    "        print(\"Starting fine-tuning...\")\n",
    "        fine_tuner.fine_tune(dataset, num_train_epochs=3, batch_size=2, chunk_size=3000)\n",
    "\n",
    "    # Perform inference on a new image\n",
    "    test_image_path = \"sample1.jpg\"  # Replace with the path to a new medical image\n",
    "    caption = fine_tuner.generate_caption(test_image_path)\n",
    "    print(f\"Generated Caption: {caption}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(load_existing_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c809cb8-f437-4d14-9a93-949a90d9e5b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
