{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9be4940b-b967-4b4a-a075-82f78a070345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import Dataset, DatasetDict, load_from_disk, concatenate_datasets\n",
    "from PIL import Image\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51da99bf-d088-4375-8c04-3cdb2c255fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROCODataLoader:\n",
    "    def __init__(self, image_dir_train, image_dir_val, annotation_file_train, annotation_file_val):\n",
    "        \"\"\"\n",
    "        Initializes the data loader with directories for training and validation images, and annotation files.\n",
    "        :param image_dir_train: Directory where training images are stored.\n",
    "        :param image_dir_val: Directory where validation images are stored.\n",
    "        :param annotation_file_train: Path to the CSV file for training annotations.\n",
    "        :param annotation_file_val: Path to the CSV file for validation annotations.\n",
    "        \"\"\"\n",
    "        self.image_dir_train = image_dir_train\n",
    "        self.image_dir_val = image_dir_val\n",
    "        self.annotation_file_train = annotation_file_train\n",
    "        self.annotation_file_val = annotation_file_val\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Loads and prepares both the training and validation datasets.\n",
    "        :return: A DatasetDict with 'train' and 'validation' datasets.\n",
    "        \"\"\"\n",
    "        train_df = pd.read_csv(self.annotation_file_train)\n",
    "        val_df = pd.read_csv(self.annotation_file_val)\n",
    "\n",
    "        # Add full image paths for training and validation sets\n",
    "        train_df['image_path'] = train_df['ID'].apply(lambda x: os.path.join(self.image_dir_train, x + \".jpg\"))\n",
    "        val_df['image_path'] = val_df['ID'].apply(lambda x: os.path.join(self.image_dir_val, x + \".jpg\"))\n",
    "\n",
    "        # Convert to Hugging Face dataset format\n",
    "        train_dataset = Dataset.from_pandas(train_df)\n",
    "        val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "        # Return a DatasetDict containing both training and validation sets\n",
    "        return DatasetDict({'train': train_dataset, 'validation': val_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41c88049-b68f-4f77-91b4-14a1b31d7162",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLIPDataCollator:\n",
    "    def __init__(self, processor, padding=True):\n",
    "        self.processor = processor\n",
    "        self.padding = padding\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # Extract text inputs (input_ids) and pixel values (images) separately\n",
    "        text_inputs = [feature['input_ids'] for feature in features]\n",
    "        labels = [feature['input_ids'] for feature in features]  # Use input_ids as labels\n",
    "        \n",
    "        # Ensure pixel_values are tensors before stacking\n",
    "        pixel_values = [torch.tensor(feature['pixel_values']) if not isinstance(feature['pixel_values'], torch.Tensor) \n",
    "                        else feature['pixel_values'] for feature in features]\n",
    "\n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "\n",
    "        # Pad the text inputs and labels\n",
    "        text_inputs = self.processor.tokenizer.pad(\n",
    "            {'input_ids': text_inputs},\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        labels = self.processor.tokenizer.pad(\n",
    "            {'input_ids': labels},\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Return the padded text, the pixel values, and the labels as a dictionary\n",
    "        return {\n",
    "            'input_ids': text_inputs['input_ids'],\n",
    "            'attention_mask': text_inputs['attention_mask'],\n",
    "            'pixel_values': pixel_values,\n",
    "            'labels': labels['input_ids']  # Add labels for loss computation\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53b49e8e-3698-44fb-be82-e6c1c0f38ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples, processor):\n",
    "    # Define the transformation pipeline (resize and convert to tensor)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize to 224x224\n",
    "        transforms.ToTensor(),  # Convert to tensor, normalizes pixel values between 0 and 1\n",
    "    ])\n",
    "\n",
    "    # Apply transformations to the images\n",
    "    images = []\n",
    "    for path in examples['image_path']:\n",
    "        try:\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "            img = transform(img)  # Apply transformation\n",
    "            images.append(img)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Stack the images into a batch and keep them on CPU initially\n",
    "    images = torch.stack(images)\n",
    "\n",
    "    # Process images separately using the processor, and set `do_rescale=False`\n",
    "    image_inputs = processor(images=images, return_tensors=\"pt\", do_rescale=False)  # Disable rescaling\n",
    "\n",
    "    # Tokenize text\n",
    "    text = [caption for caption in examples['Caption']]\n",
    "    text_inputs = processor.tokenizer(text=text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    # Move the input tensors to GPU only after pinning (CPU tensors pinned first, then moved to GPU)\n",
    "    return {\n",
    "        'input_ids': text_inputs['input_ids'].to(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        'attention_mask': text_inputs['attention_mask'].to(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        'pixel_values': image_inputs['pixel_values'].to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "514d9aa2-5727-484f-807d-f02f36565190",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLIPFineTuner:\n",
    "    def __init__(self, model_name=\"Salesforce/blip-image-captioning-base\", output_dir=\"./results\"):\n",
    "        self.processor = BlipProcessor.from_pretrained(model_name)\n",
    "        self.output_dir = output_dir\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.checkpoint_dir = \"./checkpoints\"\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    def save_checkpoint(self, epoch, optimizer, scheduler):\n",
    "        \"\"\"Save model and optimizer state after each epoch.\"\"\"\n",
    "        checkpoint_path = os.path.join(self.checkpoint_dir, f\"checkpoint_epoch_{epoch}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict() if scheduler else None\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "    def load_checkpoint(self, optimizer=None, scheduler=None):\n",
    "        \"\"\"Load model from the latest checkpoint.\"\"\"\n",
    "        checkpoints = [f for f in os.listdir(self.checkpoint_dir) if f.startswith(\"checkpoint\")]\n",
    "        if checkpoints:\n",
    "            latest_checkpoint = max(checkpoints, key=lambda x: int(x.split(\"_\")[2].split(\".\")[0]))\n",
    "            checkpoint_path = os.path.join(self.checkpoint_dir, latest_checkpoint)\n",
    "            print(f\"Loading checkpoint {checkpoint_path}...\")\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            if optimizer:\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            if scheduler and 'scheduler_state_dict' in checkpoint:\n",
    "                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            start_epoch = checkpoint['epoch'] + 1\n",
    "            print(f\"Resuming from epoch {start_epoch}\")\n",
    "            return start_epoch\n",
    "        return 0\n",
    "\n",
    "    def process_in_chunks(self, dataset, chunk_size, dataset_type):\n",
    "        processed_chunk_dir = f\"./processed_{dataset_type}_chunks/\"\n",
    "        if os.path.exists(processed_chunk_dir):\n",
    "            print(f\"Loading preprocessed {dataset_type} chunks from disk...\")\n",
    "\n",
    "            # Gather all chunk folders and load each of them\n",
    "            chunk_folders = [os.path.join(processed_chunk_dir, folder) for folder in os.listdir(processed_chunk_dir)]\n",
    "            chunk_datasets = []\n",
    "\n",
    "            for folder in chunk_folders:\n",
    "                try:\n",
    "                    chunk_datasets.append(load_from_disk(folder))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading chunk from {folder}: {e}\")\n",
    "            \n",
    "            # Concatenate all chunk datasets into a single dataset\n",
    "            print(f\"Concatenating {len(chunk_datasets)} chunks into a single dataset...\")\n",
    "            if len(chunk_datasets) > 0:\n",
    "                full_dataset = concatenate_datasets(chunk_datasets)\n",
    "                print(f\"Successfully loaded and concatenated {dataset_type} dataset.\")\n",
    "                return full_dataset\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"No valid chunks found in {processed_chunk_dir}\")\n",
    "        else:\n",
    "            print(f\"No preprocessed {dataset_type} data found. Processing now...\")\n",
    "\n",
    "            # Create a directory for processed chunks if it doesn't exist\n",
    "            os.makedirs(processed_chunk_dir, exist_ok=True)\n",
    "\n",
    "            # Process and save each chunk directly to disk to avoid memory exhaustion\n",
    "            for start_idx in range(0, len(dataset), chunk_size):\n",
    "                end_idx = min(start_idx + chunk_size, len(dataset))\n",
    "                chunk = dataset.select(range(start_idx, end_idx))\n",
    "                print(f\"Processing {dataset_type} chunk from {start_idx} to {end_idx}...\")\n",
    "                processed_chunk = chunk.map(lambda examples: preprocess_data(examples, self.processor), \n",
    "                                            batched=True, batch_size=1, load_from_cache_file=False)\n",
    "\n",
    "                # Save the chunk immediately to disk using save_to_disk()\n",
    "                chunk_path = f\"{processed_chunk_dir}/chunk_{start_idx}_{end_idx}\"\n",
    "                print(f\"Saving {dataset_type} processed chunk from {start_idx} to {end_idx} at {chunk_path}...\")\n",
    "                processed_chunk.save_to_disk(chunk_path)\n",
    "\n",
    "            # Reload the chunks after processing\n",
    "            return self.process_in_chunks(dataset, chunk_size, dataset_type)\n",
    "\n",
    "    def fine_tune(self, dataset, num_train_epochs=3, batch_size=4, chunk_size=3000):\n",
    "        if self.model is None:\n",
    "            print(f\"Loading model {self.model_name}...\")\n",
    "            self.model = BlipForConditionalGeneration.from_pretrained(self.model_name)\n",
    "\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {device}\")\n",
    "        self.model.to(device)\n",
    "\n",
    "        # Optimizer and scheduler setup\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=5e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "        # Load checkpoint if exists\n",
    "        start_epoch = self.load_checkpoint(optimizer=optimizer, scheduler=scheduler)\n",
    "\n",
    "        # Process or load the dataset in chunks to prevent memory overflow\n",
    "        print(\"Processing or loading training set in chunks...\")\n",
    "        processed_train_dataset = self.process_in_chunks(dataset[\"train\"], chunk_size, dataset_type=\"train\")\n",
    "        print(\"Processing or loading validation set in chunks...\")\n",
    "        processed_validation_dataset = self.process_in_chunks(dataset[\"validation\"], chunk_size, dataset_type=\"validation\")\n",
    "\n",
    "        # Use the custom BLIP data collator to handle padding\n",
    "        data_collator = BLIPDataCollator(self.processor)\n",
    "\n",
    "        # Set up DataLoaders manually to ensure pin_memory is disabled\n",
    "        train_dataloader = DataLoader(\n",
    "            processed_train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=False  # Disable pinning to avoid issues\n",
    "        )\n",
    "\n",
    "        validation_dataloader = DataLoader(\n",
    "            processed_validation_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=False  # Disable pinning\n",
    "        )\n",
    "\n",
    "        # Define training arguments with caching\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            eval_strategy=\"steps\",\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            save_steps=6000,\n",
    "            save_total_limit=2,\n",
    "            logging_dir=\"./logs\",\n",
    "            learning_rate=5e-5,\n",
    "            logging_steps=500,\n",
    "            eval_steps=3000,\n",
    "            remove_unused_columns=False,\n",
    "            load_best_model_at_end=True,\n",
    "            gradient_accumulation_steps=8,\n",
    "            fp16=True,  # Enable mixed precision\n",
    "            dataloader_num_workers=0,  # Disable parallel loading\n",
    "            report_to=\"none\"  # Disable W&B or other reporting\n",
    "        )\n",
    "\n",
    "        # Initialize Trainer with manually defined DataLoaders\n",
    "        print(\"Initializing Trainer...\")\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=processed_train_dataset,\n",
    "            eval_dataset=processed_validation_dataset,\n",
    "            data_collator=data_collator,\n",
    "            optimizers=(optimizer, scheduler),\n",
    "        )\n",
    "\n",
    "        # Start training\n",
    "        print(f\"Starting training from epoch {start_epoch}/{num_train_epochs}\")\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        for epoch in range(start_epoch, num_train_epochs):\n",
    "            print(f\"Training epoch {epoch}/{num_train_epochs}\")\n",
    "            trainer.train()\n",
    "\n",
    "            # Save checkpoint after each epoch\n",
    "            self.save_checkpoint(epoch, optimizer, scheduler)\n",
    "            print(f\"Checkpoint saved for epoch {epoch}\")\n",
    "\n",
    "        # Save the final fine-tuned model\n",
    "        self.save_model()\n",
    "        print(f\"Model saved at {self.output_dir}\")\n",
    "    \n",
    "    def save_model(self):\n",
    "        if self.model:\n",
    "            print(\"Saving model...\")\n",
    "            self.model.save_pretrained(self.output_dir)\n",
    "            self.processor.save_pretrained(self.output_dir)\n",
    "        else:\n",
    "            print(\"Model is not loaded, cannot save.\")\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the final fine-tuned model if it exists.\"\"\"\n",
    "        if os.path.exists(self.output_dir):\n",
    "            print(f\"Loading model from {self.output_dir}...\")\n",
    "            self.model = BlipForConditionalGeneration.from_pretrained(self.output_dir)\n",
    "            self.processor = BlipProcessor.from_pretrained(self.output_dir)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No model found at {self.output_dir}\")\n",
    "    \n",
    "    def generate_caption(self, image_path):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"The model is not loaded. Call `load_model()` or train the model first.\")\n",
    "    \n",
    "        print(f\"Generating caption for image: {image_path}\")\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        out = self.model.generate(**inputs)\n",
    "        generated_caption = self.processor.decode(out[0], skip_special_tokens=True)\n",
    "        return generated_caption   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e12624e-e782-49e4-aafb-38559e7b2114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda3\\envs\\MEDCAT\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning...\n",
      "Loading model Salesforce/blip-image-captioning-base...\n",
      "Using device: cuda\n",
      "Loading checkpoint ./checkpoints\\checkpoint_epoch_0.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ninja\\AppData\\Local\\Temp\\ipykernel_49732\\3258667818.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 31\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Caption: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaption\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 31\u001b[0m     main(load_existing_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[12], line 22\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(load_existing_model)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Fine-tune the model\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting fine-tuning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m     fine_tuner\u001b[38;5;241m.\u001b[39mfine_tune(dataset, num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3000\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Perform inference on a new image\u001b[39;00m\n\u001b[0;32m     25\u001b[0m test_image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample1.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with the path to a new medical image\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 98\u001b[0m, in \u001b[0;36mBLIPFineTuner.fine_tune\u001b[1;34m(self, dataset, num_train_epochs, batch_size, chunk_size)\u001b[0m\n\u001b[0;32m     95\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Load checkpoint if exists\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m start_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_checkpoint(optimizer\u001b[38;5;241m=\u001b[39moptimizer, scheduler\u001b[38;5;241m=\u001b[39mscheduler)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Process or load the dataset in chunks to prevent memory overflow\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing or loading training set in chunks...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 28\u001b[0m, in \u001b[0;36mBLIPFineTuner.load_checkpoint\u001b[1;34m(self, optimizer, scheduler)\u001b[0m\n\u001b[0;32m     26\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_dir, latest_checkpoint)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading checkpoint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(checkpoint_path)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer:\n",
      "File \u001b[1;32mG:\\anaconda3\\envs\\MEDCAT\\Lib\\site-packages\\torch\\serialization.py:1097\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1095\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1096\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1097\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1098\u001b[0m             opened_zipfile,\n\u001b[0;32m   1099\u001b[0m             map_location,\n\u001b[0;32m   1100\u001b[0m             pickle_module,\n\u001b[0;32m   1101\u001b[0m             overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[0;32m   1102\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1103\u001b[0m         )\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[0;32m   1105\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mG:\\anaconda3\\envs\\MEDCAT\\Lib\\site-packages\\torch\\serialization.py:1525\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# Needed for tensors where storage device and rebuild tensor device are\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# not connected (wrapper subclasses and tensors rebuilt using numpy)\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_thread_local_state\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[1;32m-> 1525\u001b[0m result \u001b[38;5;241m=\u001b[39m unpickler\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_thread_local_state\u001b[38;5;241m.\u001b[39mmap_location\n\u001b[0;32m   1528\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[1;32mG:\\anaconda3\\envs\\MEDCAT\\Lib\\site-packages\\torch\\serialization.py:1492\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1491\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1492\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[0;32m   1494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[1;32mG:\\anaconda3\\envs\\MEDCAT\\Lib\\site-packages\\torch\\serialization.py:1457\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     storage \u001b[38;5;241m=\u001b[39m overall_storage[storage_offset:storage_offset \u001b[38;5;241m+\u001b[39m numel]\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m     storage \u001b[38;5;241m=\u001b[39m zip_file\u001b[38;5;241m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[38;5;241m.\u001b[39mUntypedStorage)\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_untyped_storage\n\u001b[0;32m   1458\u001b[0m \u001b[38;5;66;03m# swap here if byteswapping is needed\u001b[39;00m\n\u001b[0;32m   1459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m byteorderdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main(load_existing_model=False):\n",
    "    # Paths to your data\n",
    "    image_dir_train = \"../Datasets/ROCO2/train_images/train\"   \n",
    "    image_dir_val = \"../Datasets/ROCO2/valid_images/valid\"\n",
    "    annotation_file_train = \"../Datasets/ROCO2/train_captions.csv\"  \n",
    "    annotation_file_val =  \"../Datasets/ROCO2/valid_captions.csv\"  \n",
    "\n",
    "    # Load the dataset\n",
    "    data_loader = ROCODataLoader(image_dir_train, image_dir_val, annotation_file_train, annotation_file_val)\n",
    "    dataset = data_loader.load_data()\n",
    "\n",
    "    # Initialize the BLIP fine-tuner\n",
    "    fine_tuner = BLIPFineTuner()\n",
    "\n",
    "    if load_existing_model:\n",
    "        # Load the saved model\n",
    "        print(\"Loading the existing model...\")\n",
    "        fine_tuner.load_model()\n",
    "    else:\n",
    "        # Fine-tune the model\n",
    "        print(\"Starting fine-tuning...\")\n",
    "        fine_tuner.fine_tune(dataset, num_train_epochs=3, batch_size=4, chunk_size=3000)\n",
    "\n",
    "    # Perform inference on a new image\n",
    "    test_image_path = \"sample1.jpg\" \n",
    "    caption = fine_tuner.generate_caption(test_image_path)\n",
    "    print(f\"Generated Caption: {caption}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(load_existing_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53ddf5b2-f8db-4f60-8520-71049190ab7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ninja\\AppData\\Local\\Temp\\ipykernel_49732\\3886240919.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_dir)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BlipForConditionalGeneration(\n",
       "  (vision_model): BlipVisionModel(\n",
       "    (embeddings): BlipVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (encoder): BlipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BlipEncoderLayer(\n",
       "          (self_attn): BlipAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): BlipMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (text_decoder): BlipTextLMHeadModel(\n",
       "    (bert): BlipTextModel(\n",
       "      (embeddings): BlipTextEmbeddings(\n",
       "        (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): BlipTextEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BlipTextLayer(\n",
       "            (attention): BlipTextAttention(\n",
       "              (self): BlipTextSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): BlipTextSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BlipTextAttention(\n",
       "              (self): BlipTextSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): BlipTextSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BlipTextIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BlipTextOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BlipTextOnlyMLMHead(\n",
       "      (predictions): BlipTextLMPredictionHead(\n",
       "        (transform): BlipTextPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=30524, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model checkpoint and processor\n",
    "checkpoint_dir = \"./checkpoints/checkpoint_epoch_0.pth\"  # Adjust to your checkpoint path\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Load model weights from the checkpoint\n",
    "checkpoint = torch.load(checkpoint_dir)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()  # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b32afa2e-e04f-483f-8993-27e3639002c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlipForConditionalGeneration(\n",
       "  (vision_model): BlipVisionModel(\n",
       "    (embeddings): BlipVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (encoder): BlipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BlipEncoderLayer(\n",
       "          (self_attn): BlipAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): BlipMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (text_decoder): BlipTextLMHeadModel(\n",
       "    (bert): BlipTextModel(\n",
       "      (embeddings): BlipTextEmbeddings(\n",
       "        (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): BlipTextEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BlipTextLayer(\n",
       "            (attention): BlipTextAttention(\n",
       "              (self): BlipTextSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): BlipTextSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BlipTextAttention(\n",
       "              (self): BlipTextSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): BlipTextSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BlipTextIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BlipTextOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BlipTextOnlyMLMHead(\n",
       "      (predictions): BlipTextLMPredictionHead(\n",
       "        (transform): BlipTextPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=30524, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "067756a0-95c4-4688-8222-daece8ac8dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: the right ankle bone is shown in this image\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess an image\n",
    "image_path = \"sample1.jpg\"   # Path to your inference image\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():  # No gradient computation is needed for inference\n",
    "    output = model.generate(**inputs)\n",
    "\n",
    "# Decode the generated output\n",
    "generated_caption = processor.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated Caption:\", generated_caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1185373-e63b-439f-b39d-2c0db1a3f723",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
