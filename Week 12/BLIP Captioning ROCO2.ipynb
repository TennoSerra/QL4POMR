{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9be4940b-b967-4b4a-a075-82f78a070345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset, DatasetDict, load_from_disk, concatenate_datasets\n",
    "from PIL import Image\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51da99bf-d088-4375-8c04-3cdb2c255fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROCODataLoader:\n",
    "    def __init__(self, image_dir_train, image_dir_val, annotation_file_train, annotation_file_val):\n",
    "        \"\"\"\n",
    "        Initializes the data loader with directories for training and validation images, and annotation files.\n",
    "        :param image_dir_train: Directory where training images are stored.\n",
    "        :param image_dir_val: Directory where validation images are stored.\n",
    "        :param annotation_file_train: Path to the CSV file for training annotations.\n",
    "        :param annotation_file_val: Path to the CSV file for validation annotations.\n",
    "        \"\"\"\n",
    "        self.image_dir_train = image_dir_train\n",
    "        self.image_dir_val = image_dir_val\n",
    "        self.annotation_file_train = annotation_file_train\n",
    "        self.annotation_file_val = annotation_file_val\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Loads and prepares both the training and validation datasets.\n",
    "        :return: A DatasetDict with 'train' and 'validation' datasets.\n",
    "        \"\"\"\n",
    "        train_df = pd.read_csv(self.annotation_file_train)\n",
    "        val_df = pd.read_csv(self.annotation_file_val)\n",
    "\n",
    "        # Add full image paths for training and validation sets\n",
    "        train_df['image_path'] = train_df['ID'].apply(lambda x: os.path.join(self.image_dir_train, x + \".jpg\"))\n",
    "        val_df['image_path'] = val_df['ID'].apply(lambda x: os.path.join(self.image_dir_val, x + \".jpg\"))\n",
    "\n",
    "        # Convert to Hugging Face dataset format\n",
    "        train_dataset = Dataset.from_pandas(train_df)\n",
    "        val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "        # Return a DatasetDict containing both training and validation sets\n",
    "        return DatasetDict({'train': train_dataset, 'validation': val_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "514d9aa2-5727-484f-807d-f02f36565190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples, processor):\n",
    "    images = []\n",
    "    for path in examples['image_path']:\n",
    "        try:\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {path}: {e}\")\n",
    "            continue\n",
    "        \n",
    "    # Truncate text that exceeds the token limit and process images\n",
    "    text = [caption for caption in examples['Caption']]\n",
    "    inputs = processor(images=images, text=text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    return {\n",
    "        'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask'],\n",
    "        'pixel_values': inputs['pixel_values']\n",
    "    }\n",
    "\n",
    "class BLIPFineTuner:\n",
    "    def __init__(self, model_name=\"Salesforce/blip-image-captioning-large\", output_dir=\"./results\"):\n",
    "        self.processor = BlipProcessor.from_pretrained(model_name)\n",
    "        self.output_dir = output_dir\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "\n",
    "    def process_in_chunks(self, dataset, chunk_size, dataset_type):\n",
    "        processed_chunk_dir = f\"./processed_{dataset_type}_chunks/\"\n",
    "        if os.path.exists(processed_chunk_dir):\n",
    "            print(f\"Loading preprocessed {dataset_type} chunks from disk...\")\n",
    "\n",
    "            # Gather all chunk folders and load each of them\n",
    "            chunk_folders = [os.path.join(processed_chunk_dir, folder) for folder in os.listdir(processed_chunk_dir)]\n",
    "            chunk_datasets = []\n",
    "\n",
    "            for folder in chunk_folders:\n",
    "                try:\n",
    "                    chunk_datasets.append(load_from_disk(folder))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading chunk from {folder}: {e}\")\n",
    "            \n",
    "            # Concatenate all chunk datasets into a single dataset\n",
    "            print(f\"Concatenating {len(chunk_datasets)} chunks into a single dataset...\")\n",
    "            if len(chunk_datasets) > 0:\n",
    "                full_dataset = concatenate_datasets(chunk_datasets)\n",
    "                print(f\"Successfully loaded and concatenated {dataset_type} dataset.\")\n",
    "                return full_dataset\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"No valid chunks found in {processed_chunk_dir}\")\n",
    "        else:\n",
    "            print(f\"No preprocessed {dataset_type} data found. Processing now...\")\n",
    "\n",
    "            # Create a directory for processed chunks if it doesn't exist\n",
    "            os.makedirs(processed_chunk_dir, exist_ok=True)\n",
    "\n",
    "            # Process and save each chunk directly to disk to avoid memory exhaustion\n",
    "            for start_idx in range(0, len(dataset), chunk_size):\n",
    "                end_idx = min(start_idx + chunk_size, len(dataset))\n",
    "                chunk = dataset.select(range(start_idx, end_idx))\n",
    "                print(f\"Processing {dataset_type} chunk from {start_idx} to {end_idx}...\")\n",
    "                processed_chunk = chunk.map(lambda examples: preprocess_data(examples, self.processor), \n",
    "                                            batched=True, batch_size=1, load_from_cache_file=False)\n",
    "\n",
    "                # Save the chunk immediately to disk using save_to_disk()\n",
    "                chunk_path = f\"{processed_chunk_dir}/chunk_{start_idx}_{end_idx}\"\n",
    "                print(f\"Saving {dataset_type} processed chunk from {start_idx} to {end_idx} at {chunk_path}...\")\n",
    "                processed_chunk.save_to_disk(chunk_path)\n",
    "\n",
    "            # Reload the chunks after processing\n",
    "            return self.process_in_chunks(dataset, chunk_size, dataset_type)\n",
    "\n",
    "    def fine_tune(self, dataset, num_train_epochs=3, batch_size=4, chunk_size=1000):\n",
    "        if self.model is None:\n",
    "            print(f\"Loading model {self.model_name}...\")\n",
    "            self.model = BlipForConditionalGeneration.from_pretrained(self.model_name)\n",
    "\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {device}\")\n",
    "        self.model.to(device)\n",
    "\n",
    "        # Process or load the dataset in chunks to prevent memory overflow\n",
    "        print(\"Processing or loading training set in chunks...\")\n",
    "        processed_train_dataset = self.process_in_chunks(dataset[\"train\"], chunk_size, dataset_type=\"train\")\n",
    "        print(\"Processing or loading validation set in chunks...\")\n",
    "        processed_validation_dataset = self.process_in_chunks(dataset[\"validation\"], chunk_size, dataset_type=\"validation\")\n",
    "\n",
    "        # Define training arguments with caching\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            save_steps=1000,\n",
    "            save_total_limit=2,\n",
    "            logging_dir=\"./logs\",\n",
    "            learning_rate=5e-5,\n",
    "            logging_steps=100,\n",
    "            eval_steps=500,\n",
    "            remove_unused_columns=False,\n",
    "            load_best_model_at_end=True\n",
    "        )\n",
    "\n",
    "        # Define the Trainer\n",
    "        print(\"Initializing Trainer...\")\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=processed_train_dataset,\n",
    "            eval_dataset=processed_validation_dataset,\n",
    "        )\n",
    "\n",
    "        # Start training\n",
    "        print(\"Starting training...\")\n",
    "        trainer.train()\n",
    "\n",
    "        # Save the fine-tuned model\n",
    "        self.save_model()\n",
    "        print(f\"Model saved at {self.output_dir}\")\n",
    "\n",
    "    def save_model(self):\n",
    "        if self.model:\n",
    "            print(\"Saving model...\")\n",
    "            self.model.save_pretrained(self.output_dir)\n",
    "            self.processor.save_pretrained(self.output_dir)\n",
    "        else:\n",
    "            print(\"Model is not loaded, cannot save.\")\n",
    "\n",
    "    def load_model(self):\n",
    "        if os.path.exists(self.output_dir):\n",
    "            print(f\"Loading model from {self.output_dir}...\")\n",
    "            self.model = BlipForConditionalGeneration.from_pretrained(self.output_dir)\n",
    "            self.processor = BlipProcessor.from_pretrained(self.output_dir)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No model found at {self.output_dir}\")\n",
    "\n",
    "    def generate_caption(self, image_path):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"The model is not loaded. Call `load_model()` or train the model first.\")\n",
    "\n",
    "        print(f\"Generating caption for image: {image_path}\")\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        out = self.model.generate(**inputs)\n",
    "        generated_caption = self.processor.decode(out[0], skip_special_tokens=True)\n",
    "        return generated_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e12624e-e782-49e4-aafb-38559e7b2114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda3\\envs\\MEDCAT\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning...\n",
      "Loading model Salesforce/blip-image-captioning-large...\n",
      "Using device: cuda\n",
      "Processing or loading training set in chunks...\n",
      "Loading preprocessed train chunks from disk...\n",
      "Concatenating 21 chunks into a single dataset...\n",
      "Successfully loaded and concatenated train dataset.\n",
      "Processing or loading validation set in chunks...\n",
      "No preprocessed validation data found. Processing now...\n",
      "Processing validation chunk from 0 to 3000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a99290d0194bcdb9691c4cc0e56de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main(load_existing_model=False):\n",
    "    # Paths to your data\n",
    "    image_dir_train = \"../Datasets/ROCO2/train_images/train\"   \n",
    "    image_dir_val = \"../Datasets/ROCO2/valid_images/valid\"\n",
    "    annotation_file_train = \"../Datasets/ROCO2/train_captions.csv\"  \n",
    "    annotation_file_val =  \"../Datasets/ROCO2/valid_captions.csv\"  \n",
    "\n",
    "    # Load the dataset\n",
    "    data_loader = ROCODataLoader(image_dir_train, image_dir_val, annotation_file_train, annotation_file_val)\n",
    "    dataset = data_loader.load_data()\n",
    "\n",
    "    # Initialize the BLIP fine-tuner\n",
    "    fine_tuner = BLIPFineTuner()\n",
    "\n",
    "    if load_existing_model:\n",
    "        # Load the saved model\n",
    "        print(\"Loading the existing model...\")\n",
    "        fine_tuner.load_model()\n",
    "    else:\n",
    "        # Fine-tune the model\n",
    "        print(\"Starting fine-tuning...\")\n",
    "        fine_tuner.fine_tune(dataset, num_train_epochs=3, batch_size=6, chunk_size=3000)\n",
    "\n",
    "    # Perform inference on a new image\n",
    "    test_image_path = \"sample1.jpg\"  # Replace with the path to a new medical image\n",
    "    caption = fine_tuner.generate_caption(test_image_path)\n",
    "    print(f\"Generated Caption: {caption}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(load_existing_model=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
