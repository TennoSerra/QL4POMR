{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e5bf3d9-28dc-45d8-9143-64336104e235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import open_clip\n",
    "from open_clip import create_model_from_pretrained, get_tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from PIL import Image\n",
    "from transformers import BertTokenizer\n",
    "from torchvision import transforms\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29361343-94f6-4729-abe9-2f79c14001b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BiomedCLIP Model, Processor, and Tokenizer\n",
    "model_name = 'hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224'\n",
    "model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms(model_name)\n",
    "tokenizer = get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27ba46e5-7348-444b-85cc-88079bf301a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to pre-split datasets\n",
    "train_caption_file = \"../Datasets/ROCO2/train_captions.csv\"  # Assuming this file contains training data\n",
    "val_caption_file = \"../Datasets/ROCO2/valid_captions.csv\"      # Assuming this file contains validation data\n",
    "test_caption_file = \"../Datasets/ROCO2/test_captions.csv\"    # Assuming this file contains testing data\n",
    "\n",
    "# Define paths to image folders\n",
    "train_image_folder = \"../Datasets/ROCO2/train_images/train/\"\n",
    "val_image_folder = \"../Datasets/ROCO2/valid_images/valid/\"\n",
    "test_image_folder = \"../Datasets/ROCO2/test_images/test/\"\n",
    "\n",
    "# Read the text files into DataFrames\n",
    "train_df = pd.read_csv(train_caption_file, sep=',', header=1, names=['ID', 'Caption'])\n",
    "val_df = pd.read_csv(val_caption_file, sep=',', header=1, names=['ID', 'Caption'])\n",
    "test_df = pd.read_csv(test_caption_file, sep=',', header=1, names=['ID', 'Caption'])\n",
    "\n",
    "# Replace the ID column with the Image column\n",
    "train_df['Image'] = train_df['ID'] + \".jpg\"\n",
    "val_df['Image'] = val_df['ID'] + \".jpg\"\n",
    "test_df['Image'] = test_df['ID'] + \".jpg\"\n",
    "\n",
    "# Drop the old ID column\n",
    "train_df.drop('ID', axis=1, inplace=True)\n",
    "val_df.drop('ID', axis=1, inplace=True)\n",
    "test_df.drop('ID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41a4cd98-e05f-43ff-927a-b30c5af247f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing steps for images\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "context_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acb67b6b-1dfc-4c5e-843a-da04d8f9320d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionImageDataset(Dataset):\n",
    "    def __init__(self, captions_df, image_folder, tokenizer, preprocess, context_length):\n",
    "        self.captions_df = captions_df\n",
    "        self.image_folder = image_folder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.preprocess = preprocess\n",
    "        self.context_length = context_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = Path(self.image_folder) / self.captions_df.iloc[idx, 0]\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        image = self.preprocess(image)\n",
    "        \n",
    "        caption = self.captions_df.iloc[idx, 1]\n",
    "        text = self.tokenizer(caption, context_length=self.context_length)\n",
    "        \n",
    "        return image, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bee22475-9102-4141-9697-ec31e7079635",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CaptionImageDataset(train_df, train_image_folder, tokenizer, preprocess, context_length)\n",
    "val_dataset = CaptionImageDataset(val_df, val_image_folder, tokenizer, preprocess, context_length)\n",
    "test_dataset = CaptionImageDataset(test_df, test_image_folder, tokenizer, preprocess, context_length)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67193d85-1d03-4194-80d3-a9301af73b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train_one_epoch(model, data_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, texts in data_loader:\n",
    "        images = images.to(device)\n",
    "        texts = texts.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        image_features, text_features, logit_scale = model(images, texts)\n",
    "        logits = (logit_scale * image_features @ text_features.t()).detach().softmax(dim=-1)\n",
    "        \n",
    "        loss = -torch.log(logits).mean()  # Example loss function\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, texts in data_loader:\n",
    "            images = images.to(device)\n",
    "            texts = texts.to(device)\n",
    "            \n",
    "            image_features, text_features, logit_scale = model(images, texts)\n",
    "            logits = (logit_scale * image_features @ text_features.t()).detach().softmax(dim=-1)\n",
    "            \n",
    "            all_logits.append(logits.cpu().numpy())\n",
    "            all_labels.append(texts.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(all_logits), np.concatenate(all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11c3c643-2fdf-4e08-845f-61c77131c859",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ellipsis' object has no attribute 'parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize model, optimizer\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m  \u001b[38;5;66;03m# Load or define your model\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Train for several epochs\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ellipsis' object has no attribute 'parameters'"
     ]
    }
   ],
   "source": [
    "# Initialize model, optimizer\n",
    "model = ...  # Load or define your model\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Train for several epochs\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Optionally evaluate on validation set\n",
    "    val_logits, val_labels = evaluate(model, val_loader, device)\n",
    "    # Calculate metrics, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc9ab76-164c-4b85-bd31-f7779ffc38c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
