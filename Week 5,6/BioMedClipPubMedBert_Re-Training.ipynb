{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e5bf3d9-28dc-45d8-9143-64336104e235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda3\\envs\\ModeEval\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import open_clip\n",
    "from open_clip import create_model_from_pretrained, get_tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from PIL import Image\n",
    "from transformers import BertTokenizer\n",
    "from torchvision import transforms\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29361343-94f6-4729-abe9-2f79c14001b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BiomedCLIP Model, Processor, and Tokenizer\n",
    "model_name = 'hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224'\n",
    "model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms(model_name)\n",
    "tokenizer = get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27ba46e5-7348-444b-85cc-88079bf301a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to pre-split datasets\n",
    "train_caption_file = \"../Datasets/ROCO2/train_captions.csv\"  # Assuming this file contains training data\n",
    "val_caption_file = \"../Datasets/ROCO2/valid_captions.csv\"      # Assuming this file contains validation data\n",
    "test_caption_file = \"../Datasets/ROCO2/test_captions.csv\"    # Assuming this file contains testing data\n",
    "\n",
    "# Define paths to image folders\n",
    "train_image_folder = \"../Datasets/ROCO2/train_images/train/\"\n",
    "val_image_folder = \"../Datasets/ROCO2/valid_images/valid/\"\n",
    "test_image_folder = \"../Datasets/ROCO2/test_images/test/\"\n",
    "\n",
    "# Read the text files into DataFrames\n",
    "train_df = pd.read_csv(train_caption_file, sep=',', header=1, names=['ID', 'Caption'])\n",
    "val_df = pd.read_csv(val_caption_file, sep=',', header=1, names=['ID', 'Caption'])\n",
    "test_df = pd.read_csv(test_caption_file, sep=',', header=1, names=['ID', 'Caption'])\n",
    "\n",
    "# Replace the ID column with the Image column\n",
    "train_df['Image'] = train_df['ID'] + \".jpg\"\n",
    "val_df['Image'] = val_df['ID'] + \".jpg\"\n",
    "test_df['Image'] = test_df['ID'] + \".jpg\"\n",
    "\n",
    "# Drop the old ID column\n",
    "train_df.drop('ID', axis=1, inplace=True)\n",
    "val_df.drop('ID', axis=1, inplace=True)\n",
    "test_df.drop('ID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41a4cd98-e05f-43ff-927a-b30c5af247f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing steps for images\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "context_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acb67b6b-1dfc-4c5e-843a-da04d8f9320d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionImageDataset(Dataset):\n",
    "    def __init__(self, captions_df, image_folder, tokenizer, preprocess, context_length):\n",
    "        self.captions_df = captions_df\n",
    "        self.image_folder = image_folder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.preprocess = preprocess\n",
    "        self.context_length = context_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = Path(self.image_folder) / self.captions_df.iloc[idx, 0]\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        image = self.preprocess(image)\n",
    "        \n",
    "        caption = self.captions_df.iloc[idx, 1]\n",
    "        text = self.tokenizer(caption, context_length=self.context_length)\n",
    "        \n",
    "        return image, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bee22475-9102-4141-9697-ec31e7079635",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CaptionImageDataset(train_df, train_image_folder, tokenizer, preprocess_train, context_length)\n",
    "val_dataset = CaptionImageDataset(val_df, val_image_folder, tokenizer, preprocess_val, context_length)\n",
    "test_dataset = CaptionImageDataset(test_df, test_image_folder, tokenizer, preprocess_val, context_length)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67193d85-1d03-4194-80d3-a9301af73b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_one_epoch(model, data_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, texts in data_loader:\n",
    "        images = images.to(device)\n",
    "        texts = texts.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        image_features, text_features = model(images, texts)\n",
    "        \n",
    "        logits = torch.matmul(image_features, text_features.t())  # Calculate logits\n",
    "        logits = logits.softmax(dim=-1)\n",
    "        \n",
    "        # Example loss calculation; adjust as needed\n",
    "        loss = -torch.log(logits.diag()).mean()  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, texts in data_loader:\n",
    "            images = images.to(device)\n",
    "            texts = texts.to(device)\n",
    "            \n",
    "            image_features, text_features = model(images, texts)\n",
    "            logits = torch.matmul(image_features, text_features.t()).softmax(dim=-1)\n",
    "            \n",
    "            all_logits.append(logits.cpu().numpy())\n",
    "            all_labels.append(texts.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(all_logits), np.concatenate(all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dae0611b-d531-4f9e-bc4f-3cff9837212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def save_embeddings(model, data_loader, device, output_file):\n",
    "    model.eval()\n",
    "    all_image_embeddings = []\n",
    "    all_text_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for images, texts in data_loader:\n",
    "            images = images.to(device)\n",
    "            texts = texts.to(device)\n",
    "            \n",
    "            image_features, text_features = model(images, texts)\n",
    "            \n",
    "            all_image_embeddings.append(image_features.cpu().numpy())\n",
    "            all_text_embeddings.append(text_features.cpu().numpy())\n",
    "    \n",
    "    all_image_embeddings = np.concatenate(all_image_embeddings, axis=0)\n",
    "    all_text_embeddings = np.concatenate(all_text_embeddings, axis=0)\n",
    "    \n",
    "    np.savez(output_file, image_embeddings=all_image_embeddings, text_embeddings=all_text_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c3c643-2fdf-4e08-845f-61c77131c859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# Initialize model and optimizer\n",
    "model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms(model_name)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "num_epochs = 10  # Number of epochs for training\n",
    "# Set the path for saving embeddings\n",
    "embedding_output_file = 'BioMedClipPubMedBERT_embeddings.npz'\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Optionally evaluate on validation set\n",
    "    val_logits, val_labels = evaluate(model, val_loader, device)\n",
    "    # Calculate metrics, etc.\n",
    "\n",
    "# Save embeddings from the training set\n",
    "save_embeddings(model, train_loader, device, embedding_output_file)\n",
    "\n",
    "# Save embeddings from the validation set\n",
    "save_embeddings(model, val_loader, device, 'val_embeddings.npz')\n",
    "\n",
    "# Optionally save embeddings from the test set\n",
    "save_embeddings(model, test_loader, device, 'test_embeddings.npz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc9ab76-164c-4b85-bd31-f7779ffc38c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
