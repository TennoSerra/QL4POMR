{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69b01a53-2c8f-4cbe-91fc-163c11e59c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41327329-8e2d-43f8-9e62-c8b95e943e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the learning rate for the optimizer\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Image size\n",
    "image_size = 128\n",
    "\n",
    "# Define the number of epochs for training\n",
    "nepochs = 200\n",
    "\n",
    "# Define the batch size for mini-batch gradient descent\n",
    "batch_size = 128\n",
    "\n",
    "# Define the root directory of the dataset\n",
    "data_set_root = '../Datasets/ROCO2'\n",
    "train_captions_file = os.path.join(data_set_root, 'train_captions.csv')\n",
    "test_captions_file = os.path.join(data_set_root, 'test_captions.csv')\n",
    "train_image_path = os.path.join(data_set_root, 'train_images/train/')\n",
    "test_image_path = os.path.join(data_set_root, 'test_images/test/')\n",
    "\n",
    "# ## Data processing and Tokenization\n",
    "\n",
    "class SampleCaption(nn.Module):\n",
    "    def __call__(self, sample):\n",
    "        rand_index = random.randint(0, len(sample) - 1)\n",
    "        return sample[rand_index]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.RandomCrop(image_size),\n",
    "    transforms.AutoAugment(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create a custom dataset for ROCO 2\n",
    "class ROCO2Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_dir, captions_file, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.captions_df = pd.read_csv(captions_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_dir, self.captions_df.iloc[idx, 0])\n",
    "        image = torchvision.io.read_image(img_name)\n",
    "        captions = self.captions_df.iloc[idx, 1].split('|')  # Assuming captions are separated by '|'\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, captions\n",
    "\n",
    "train_dataset = ROCO2Dataset(train_image_path, train_captions_file, transform=train_transform)\n",
    "eval_dataset = ROCO2Dataset(test_image_path, test_captions_file, transform=transform)\n",
    "\n",
    "data_loader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "data_loader_eval = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "# Create a dataloader iterable object\n",
    "dataiter = next(iter(data_loader_eval))\n",
    "test_images, test_captions = dataiter\n",
    "\n",
    "index = 0\n",
    "# # Let's visualize an entire batch of images!\n",
    "# plt.figure(figsize=(3, 3))\n",
    "# out = torchvision.utils.make_grid(test_images[index].unsqueeze(0), 1, normalize=True)\n",
    "# _ = plt.imshow(out.numpy().transpose((1, 2, 0)))\n",
    "\n",
    "caption = test_captions[index]\n",
    "print(caption)\n",
    "\n",
    "# We'll use a pre-built Tokenizer for the BERT Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "tokens = tokenizer(test_captions, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "token_ids = tokens['input_ids'][index]\n",
    "print(tokenizer.decode(token_ids))\n",
    "\n",
    "class TokenDrop(nn.Module):\n",
    "    def __init__(self, prob=0.1, blank_token=1, eos_token=102):\n",
    "        self.prob = prob\n",
    "        self.eos_token = eos_token\n",
    "        self.blank_token = blank_token\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        mask = torch.bernoulli(self.prob * torch.ones_like(sample)).long()\n",
    "        can_drop = (~(sample == self.eos_token)).long()\n",
    "        mask = mask * can_drop\n",
    "        mask[:, 0] = torch.zeros_like(mask[:, 0]).long()\n",
    "        replace_with = (self.blank_token * torch.ones_like(sample)).long()\n",
    "        sample_out = (1 - mask) * sample + mask * replace_with\n",
    "        return sample_out\n",
    "\n",
    "def extract_patches(image_tensor, patch_size=16):\n",
    "    bs, c, h, w = image_tensor.size()\n",
    "    unfold = torch.nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "    unfolded = unfold(image_tensor)\n",
    "    unfolded = unfolded.transpose(1, 2).reshape(bs, -1, c * patch_size * patch_size)\n",
    "    return unfolded\n",
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_emb, hidden_size=128, num_layers=3, num_heads=4):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_emb, hidden_size)\n",
    "        self.embedding.weight.data = 0.001 * self.embedding.weight.data\n",
    "        self.pos_emb = SinusoidalPosEmb(hidden_size)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=hidden_size, nhead=num_heads,\n",
    "                                                   dim_feedforward=hidden_size * 4, dropout=0.0,\n",
    "                                                   batch_first=True)\n",
    "        self.decoder_layers = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(hidden_size, num_emb)\n",
    "\n",
    "    def forward(self, input_seq, encoder_output, input_padding_mask=None,\n",
    "                encoder_padding_mask=None):\n",
    "        input_embs = self.embedding(input_seq)\n",
    "        bs, l, h = input_embs.shape\n",
    "        seq_indx = torch.arange(l, device=input_seq.device)\n",
    "        pos_emb = self.pos_emb(seq_indx).reshape(1, l, h).expand(bs, l, h)\n",
    "        embs = input_embs + pos_emb\n",
    "        causal_mask = torch.triu(torch.ones(l, l, device=input_seq.device), 1).bool()\n",
    "        output = self.decoder_layers(tgt=embs, memory=encoder_output, tgt_mask=causal_mask,\n",
    "                                     tgt_key_padding_mask=input_padding_mask,\n",
    "                                     memory_key_padding_mask=encoder_padding_mask)\n",
    "        return self.fc_out(output)\n",
    "\n",
    "class VisionEncoder(nn.Module):\n",
    "    def __init__(self, image_size, channels_in, patch_size=16, hidden_size=128, num_layers=3, num_heads=4):\n",
    "        super(VisionEncoder, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.fc_in = nn.Linear(channels_in * patch_size * patch_size, hidden_size)\n",
    "        seq_length = (image_size // patch_size) ** 2\n",
    "        self.pos_embedding = nn.Parameter(torch.empty(1, seq_length, hidden_size).normal_(std=0.02))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=num_heads,\n",
    "                                                   dim_feedforward=hidden_size * 4, dropout=0.0,\n",
    "                                                   batch_first=True)\n",
    "        self.encoder_layers = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "    def forward(self, image):\n",
    "        bs = image.shape[0]\n",
    "        patch_seq = extract_patches(image, patch_size=self.patch_size)\n",
    "        patch_emb = self.fc_in(patch_seq)\n",
    "        embs = patch_emb + self.pos_embedding\n",
    "        output = self.encoder_layers(embs)\n",
    "        return output\n",
    "\n",
    "class VisionEncoderDecoder(nn.Module):\n",
    "    def __init__(self, image_size, channels_in, num_emb, patch_size=16,\n",
    "                 hidden_size=128, num_layers=(3, 3), num_heads=4):\n",
    "        super(VisionEncoderDecoder, self).__init__()\n",
    "        self.encoder = VisionEncoder(image_size=image_size, channels_in=channels_in, patch_size=patch_size,\n",
    "                                     hidden_size=hidden_size, num_layers=num_layers[0], num_heads=num_heads)\n",
    "        self.decoder = Decoder(num_emb=num_emb, hidden_size=hidden_size,\n",
    "                               num_layers=num_layers[1], num_heads=num_heads)\n",
    "\n",
    "    def forward(self, input_image, target_seq, padding_mask):\n",
    "        bool_padding_mask = padding_mask == 0\n",
    "        encoded_seq = self.encoder(image=input_image)\n",
    "        decoded_seq = self.decoder(input_seq=target_seq,\n",
    "                                   encoder_output=encoded_seq,\n",
    "                                   input_padding_mask=bool_padding_mask)\n",
    "        return decoded_seq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45a86ae-2cb4-42c6-bea3-a20c0d5b8ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Initialise Model and Optimizer\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Define number of unique tokens (vocabulary size)\n",
    "num_tokens = tokenizer.vocab_size\n",
    "\n",
    "# Initialize the model\n",
    "model = VisionEncoderDecoder(image_size=image_size,\n",
    "                             channels_in=3,  # RGB channels\n",
    "                             num_emb=num_tokens,\n",
    "                             hidden_size=128,\n",
    "                             num_layers=(3, 3),\n",
    "                             num_heads=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab44b89-4d1a-487c-9683-56e10b519b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Loss Function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Directory for saving checkpoints\n",
    "checkpoint_dir = './checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# ## Training Loop\n",
    "# Start training\n",
    "for epoch in range(nepochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (images, captions) in tqdm(enumerate(data_loader_train)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Convert the images and captions to tensors and move to device\n",
    "        images = images.to(device)\n",
    "        target_seq = tokenizer(captions, padding=True, truncation=True, return_tensors=\"pt\")['input_ids'].to(device)\n",
    "        padding_mask = (target_seq == 0)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_image=images, target_seq=target_seq[:, :-1], padding_mask=padding_mask[:, 1:])\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs.reshape(-1, outputs.size(-1)), target_seq[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if (i + 1) % 10 == 0:  # Print every 10 batches\n",
    "            print(f\"Epoch [{epoch + 1}/{nepochs}], Step [{i + 1}/{len(data_loader_train)}], Loss: {running_loss / 10:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Save checkpoint at the end of each epoch\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch + 1}.pth')\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss.item(),  # Optionally save the loss if needed\n",
    "    }, checkpoint_path)\n",
    "\n",
    "    print(f\"Checkpoint saved: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88f9dc8-f02b-46a5-aefe-c4aedc59cea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Evaluation\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_loss = 0.0\n",
    "    for images, captions in data_loader_eval:\n",
    "        images = images.to(device)\n",
    "        target_seq = tokenizer(captions, padding=True, truncation=True, return_tensors=\"pt\")['input_ids'].to(device)\n",
    "        padding_mask = (target_seq == 0)\n",
    "\n",
    "        outputs = model(input_image=images, target_seq=target_seq[:, :-1], padding_mask=padding_mask[:, 1:])\n",
    "        \n",
    "        loss = criterion(outputs.reshape(-1, outputs.size(-1)), target_seq[:, 1:].reshape(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Evaluation Loss: {total_loss / len(data_loader_eval):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522fb979-4bce-47a3-8f1a-e23d88420edd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4571d5b-5919-4460-a7d4-6cf8d368f73a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
